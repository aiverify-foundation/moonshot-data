{
  "squad-shifts-tnf": {
    "id": "squad-shifts-tnf",
    "name": "squad-shifts-tnf",
    "description": "Zero-shot reading comprehension on paragraphs and questions from squadshifts",
    "examples": null,
    "num_of_dataset_prompts": 48201,
    "created_date": "2025-05-19 18:10:28",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/squad_shifts",
    "license": "Apache 2.0",
    "hash": "5dfcdfeb4fd1ddaa"
  },
  "bbq-lite-religion-ambiguous": {
    "id": "bbq-lite-religion-ambiguous",
    "name": "BBQ-lite on religion - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 600,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "d2a0f4023c20d4fd"
  },
  "advglue-all": {
    "id": "advglue-all",
    "name": "advglue",
    "description": "Adversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark that focuses on the adversarial robustness evaluation of language models. ",
    "examples": null,
    "num_of_dataset_prompts": 721,
    "created_date": "2025-05-19 18:05:40",
    "reference": "https://github.com/AI-secure/adversarial-glue",
    "license": "CC-BY-4.0 license",
    "hash": "e7f97daedfd0252c"
  },
  "singapore-food-tnf": {
    "id": "singapore-food-tnf",
    "name": "Food in Singapore",
    "description": "Contain prompts that test model's udnerstanding in Food, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "675fc75234a5932a"
  },
  "medmcqa": {
    "id": "medmcqa",
    "name": "MedMCQA",
    "description": "MedMCQ is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects.",
    "examples": null,
    "num_of_dataset_prompts": 182822,
    "created_date": "2025-06-06 18:16:49",
    "reference": "https://github.com/MedMCQA/MedMCQA",
    "license": "MIT License",
    "hash": "1991e017f5ac96b0"
  },
  "tamil-kural-classification": {
    "id": "tamil-kural-classification",
    "name": "tamil-thirukural",
    "description": "This dataset is used to test the comprehension abilities for the Thirukkural. Thirukkural is a classic Tamil literature composed by the ancient Tamil poet Thiruvalluvar. It consists of 1330 couplets (kurals) that are grouped into 133 chapters, each containing 10 couplets.",
    "examples": null,
    "num_of_dataset_prompts": 266,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/vijayanandrp/Thirukkural-Tamil-Dataset",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "52589869fc432c79"
  },
  "realtimeqa-past": {
    "id": "realtimeqa-past",
    "name": "RealtimeQA",
    "description": "RealTime QA is a dynamic question answering (QA) platform that inquires about the present. ",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/realtimeqa/realtimeqa_public",
    "license": "",
    "hash": "d4295a228fdf9a15"
  },
  "sg-university-tutorial-questions-legal": {
    "id": "sg-university-tutorial-questions-legal",
    "name": "sg-university-tutorial-questions-legal",
    "description": "Contain tutorial questions ans answers from Singapore's Universities to test model's ability in understanding legal context in Singapore",
    "examples": null,
    "num_of_dataset_prompts": 32,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "06c2951c6bd3c505"
  },
  "cbbq-lite-disease-ambiguous": {
    "id": "cbbq-lite-disease-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 586,
    "created_date": "2025-05-19 18:06:46",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "00c14e175a680598"
  },
  "bbq-lite-gender-disamb": {
    "id": "bbq-lite-gender-disamb",
    "name": "BBQ-lite on gender - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 2836,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "a0202b8a72ed15d1"
  },
  "arc-challenge": {
    "id": "arc-challenge",
    "name": "ARC-Easy",
    "description": "A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the challenge set.",
    "examples": null,
    "num_of_dataset_prompts": 2590,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://allenai.org/data/arc",
    "license": "CC BY-SA",
    "hash": "f9e244daec327295"
  },
  "bbq-lite-nationality-ambiguous": {
    "id": "bbq-lite-nationality-ambiguous",
    "name": "BBQ-lite on nationality - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 1540,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c4f51ff728a92d6f"
  },
  "bbq-lite-physical-appearance-disamb": {
    "id": "bbq-lite-physical-appearance-disamb",
    "name": "BBQ-lite on physical-appearance - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 788,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c9598647ba16b5b0"
  },
  "tamil-tanglish-tweets": {
    "id": "tamil-tanglish-tweets",
    "name": "tanglish-tweets-SA",
    "description": "Code-mixed Tamil and English tweets curated for the sentiment analysis task.",
    "examples": null,
    "num_of_dataset_prompts": 1163,
    "created_date": "2025-01-15 09:09:08",
    "reference": "https://www.kaggle.com/datasets/vyombhatia/tanglish-comments-for-sentiment-ananlysis/data",
    "license": "CC0: Public Domain",
    "hash": "470d4ed0ca26a599"
  },
  "commonsense-morality-easy-variation2": {
    "id": "commonsense-morality-easy-variation2",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "c0ebcdfa06917e0b"
  },
  "fairness-uciadult": {
    "id": "fairness-uciadult",
    "name": "uciadult",
    "description": "The UCI Adult dataset has been used widely used to assess fairness.",
    "examples": null,
    "num_of_dataset_prompts": 32561,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://archive.ics.uci.edu/dataset/2/adult",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "9066896dc1f5eeb8"
  },
  "bbq-lite-age-disamb": {
    "id": "bbq-lite-age-disamb",
    "name": "BBQ-lite on age - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on age, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 1840,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "a98d5b0508b95eb1"
  },
  "mlc-ssh-typical-user": {
    "id": "mlc-ssh-typical-user",
    "name": "MLC Baseline - Suicide & Self-Harm",
    "description": "Responses that enable, encourage, or endorse acts of intentional self-harm.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:07:41",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "bf372c2d186fd8d8"
  },
  "sg-legal-glossary": {
    "id": "sg-legal-glossary",
    "name": "sg-legal-glossary",
    "description": "A list of singapore legal terms extracted from SICC and Judiciary websites.",
    "examples": null,
    "num_of_dataset_prompts": 425,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://www.sicc.gov.sg/glossary-of-legal-terms",
    "license": "",
    "hash": "b871a0126dcb0ef1"
  },
  "mlc-hat-malicious-user": {
    "id": "mlc-hat-malicious-user",
    "name": "MLC Baseline - Hate",
    "description": "Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics.",
    "examples": null,
    "num_of_dataset_prompts": 26530,
    "created_date": "2025-06-03 16:05:48",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "942ea814feb15842"
  },
  "bbq-lite-age-ambiguous": {
    "id": "bbq-lite-age-ambiguous",
    "name": "BBQ-lite on age - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on age, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 1840,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "2c3dc041fa7fc465"
  },
  "cause-and-effect": {
    "id": "cause-and-effect",
    "name": "cause_and_effect",
    "description": "This recipe measures if the model is able to distinguish and understand cause and effect. The higher the grade, the better the system can perform this capability.",
    "examples": null,
    "num_of_dataset_prompts": 102,
    "created_date": "2025-05-19 18:06:13",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cause_and_effect",
    "license": "Apache 2.0",
    "hash": "d94e1c745917be2c"
  },
  "bbq-lite-race-x-gender-ambiguous": {
    "id": "bbq-lite-race-x-gender-ambiguous",
    "name": "BBQ-lite on race-x-gender - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 7980,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "650beb60bfe9499d"
  },
  "cbbq-lite-gender-disamb": {
    "id": "cbbq-lite-gender-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1534,
    "created_date": "2025-05-19 18:07:12",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "a6aa84d4fa221098"
  },
  "truthfulqa-mcq": {
    "id": "truthfulqa-mcq",
    "name": "TruthfulQA (MCQ Version)",
    "description": "TruthfulQA aims to measure the the truthfulness of a model. This dataset uses a multiple choice format.",
    "examples": null,
    "num_of_dataset_prompts": 483,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "91e51c0467028c0c"
  },
  "commonsense-morality-hard-variation2": {
    "id": "commonsense-morality-hard-variation2",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "4ab07a65d58af4be"
  },
  "pubmedqa": {
    "id": "pubmedqa",
    "name": "PubMedQA",
    "description": "PubMedQA is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrance exam MCQs covering 2.4k healthcare topics and 21 medical subjects.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2025-06-06 18:18:35",
    "reference": "https://github.com/MedMCQA/MedMCQA",
    "license": "MIT License",
    "hash": "738c3d8d60a7d424"
  },
  "singapore-facts-tnf": {
    "id": "singapore-facts-tnf",
    "name": "Facts about Singapore in True and False",
    "description": "Contain prompts that contains facts about Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "ce5cbbf1556b2227"
  },
  "cbbq-lite-household-registration-ambiguous": {
    "id": "cbbq-lite-household-registration-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 8700,
    "created_date": "2025-05-19 18:07:15",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "8bfaa27ccf6382e2"
  },
  "real-toxicity-prompts": {
    "id": "real-toxicity-prompts",
    "name": "RealToxicityPrompts",
    "description": "RealToxicityPrompts aims to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 198884,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/allenai/real-toxicity-prompts/",
    "license": "Apache-2.0",
    "hash": "79626be4ef13e622"
  },
  "cbbq-lite-race-disamb": {
    "id": "cbbq-lite-race-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-05-19 18:07:47",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "4a982c1a5d1fcbe4"
  },
  "gre-reading-comprehension": {
    "id": "gre-reading-comprehension",
    "name": "gre_reading_comprehension",
    "description": "To measure the model/'s ability to summarize text, infer and deduce knowledge from context, and match the context.",
    "examples": null,
    "num_of_dataset_prompts": 32,
    "created_date": "2025-05-19 18:09:34",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gre_reading_comprehension",
    "license": "Apache 2.0",
    "hash": "570b43e579ded05e"
  },
  "mmlu-collegemedicine": {
    "id": "mmlu-collegemedicine",
    "name": "MMLU College Medicine",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Medicine",
    "examples": null,
    "num_of_dataset_prompts": 173,
    "created_date": "2025-05-19 14:30:11",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "236c9bf16ddb2351"
  },
  "bbq-lite-race-x-ses-disamb": {
    "id": "bbq-lite-race-x-ses-disamb",
    "name": "BBQ-lite on race-x-ses - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 5580,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "8c85114fb0b3a15f"
  },
  "cbbq-lite-educational-qualification-disamb": {
    "id": "cbbq-lite-educational-qualification-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1392,
    "created_date": "2025-05-19 18:06:56",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "a558488c7964395e"
  },
  "singapore-political-history": {
    "id": "singapore-political-history",
    "name": "Singapore Polical History",
    "description": "Contain questions about Singapore's key historical events in political.",
    "examples": null,
    "num_of_dataset_prompts": 21,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "406e0a64815cb329"
  },
  "challenging-toxicity-prompts-variation2": {
    "id": "challenging-toxicity-prompts-variation2",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "d3fe1d18b0bfd24d"
  },
  "mlc-vcr-malicious-user": {
    "id": "mlc-vcr-malicious-user",
    "name": "MLC Baseline - Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 4390,
    "created_date": "2025-06-03 16:08:13",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "a0dd07da169640d1"
  },
  "cbbq-lite-SES-disamb": {
    "id": "cbbq-lite-SES-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 3960,
    "created_date": "2025-05-19 18:08:09",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "ff216840e0a7626e"
  },
  "mmlu-clinicalknowledge": {
    "id": "mmlu-clinicalknowledge",
    "name": "MMLU Clinical Knowledge",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Clinical Knowledge",
    "examples": null,
    "num_of_dataset_prompts": 265,
    "created_date": "2025-05-19 14:30:00",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "1f7a9e21f4b0689d"
  },
  "cbbq-lite-religion-disamb": {
    "id": "cbbq-lite-religion-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-05-19 18:08:01",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "6c7b712114eac898"
  },
  "coqa-conversational-qna": {
    "id": "coqa-conversational-qna",
    "name": "coqa_conversational_question_answering",
    "description": "To measure the ability of machines to understand a text passage and answer a series of interconnected questions",
    "examples": null,
    "num_of_dataset_prompts": 10930,
    "created_date": "2025-05-19 18:09:02",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/coqa_conversational_question_answering",
    "license": "Apache 2.0",
    "hash": "c7fd823aeffae179"
  },
  "singapore-public-housing": {
    "id": "singapore-public-housing",
    "name": "Singapore Transport System",
    "description": "Contain questions about Singapore's public housing system",
    "examples": null,
    "num_of_dataset_prompts": 12,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "45557b9f992e2440"
  },
  "challenging-toxicity-prompts": {
    "id": "challenging-toxicity-prompts",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 2392,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "e678c26593f89c20"
  },
  "bbq-lite-race-x-ses-ambiguous": {
    "id": "bbq-lite-race-x-ses-ambiguous",
    "name": "BBQ-lite on race-x-ses - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 5580,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "b2f1b70a2df153b1"
  },
  "mlc-cbr-malicious-user": {
    "id": "mlc-cbr-malicious-user",
    "name": "MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives",
    "description": "Responses that enable, encourage, or endorse the creation of weapons of mass destruction.",
    "examples": null,
    "num_of_dataset_prompts": 1570,
    "created_date": "2025-06-03 16:05:37",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "b22e223252d05835"
  },
  "bbq-lite-gender-ambiguous": {
    "id": "bbq-lite-gender-ambiguous",
    "name": "BBQ-lite on gender - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 2836,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "7aa2dc66e11ac610"
  },
  "bbq-lite-sexual-orientation-ambiguous": {
    "id": "bbq-lite-sexual-orientation-ambiguous",
    "name": "BBQ-lite on sexual-orientation - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "cf874fd902d294db"
  },
  "cbbq-lite-age-ambiguous": {
    "id": "cbbq-lite-age-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 7400,
    "created_date": "2025-05-19 18:06:24",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "4f4656aa00393667"
  },
  "mlc-cbr-typical-user": {
    "id": "mlc-cbr-typical-user",
    "name": "MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives",
    "description": "Responses that enable, encourage, or endorse the creation of weapons of mass destruction.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:05:42",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "1387e02e8e1c379b"
  },
  "mlc-hat-typical-user": {
    "id": "mlc-hat-typical-user",
    "name": "MLC Baseline - Hate",
    "description": "Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:05:53",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "b7d0f3437036c918"
  },
  "cbbq-lite-nationality-ambiguous": {
    "id": "cbbq-lite-nationality-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 11988,
    "created_date": "2025-05-19 18:07:23",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "3dfa103cc6aaa58d"
  },
  "cbbq-lite-physical-appearance-disamb": {
    "id": "cbbq-lite-physical-appearance-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1856,
    "created_date": "2025-05-19 18:07:39",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "05918182f71a8748"
  },
  "cbbq-lite-region-disamb": {
    "id": "cbbq-lite-region-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 2176,
    "created_date": "2025-05-19 18:07:53",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "206c1dfb98a3c8ea"
  },
  "winogrande": {
    "id": "winogrande",
    "name": "Winogrande",
    "description": "This dataset is used for commonsense reasoning, expert-crafted pronoun resolution problems designed to be unsolvable for statistical models.",
    "examples": null,
    "num_of_dataset_prompts": 41665,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/allenai/winogrande",
    "license": "Apache-2.0",
    "hash": "21ad250f9c027033"
  },
  "cbbq-lite-religion-ambiguous": {
    "id": "cbbq-lite-religion-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-05-19 18:07:58",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "6c7b712114eac898"
  },
  "contextual-parametric-knowledge-conflicts": {
    "id": "contextual-parametric-knowledge-conflicts",
    "name": "Contextual Parametric Knowledge Conflicts",
    "description": "To measure the model/’s ability in making informed predictions",
    "examples": null,
    "num_of_dataset_prompts": 17528,
    "created_date": "2025-05-19 18:08:48",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/contextual_parametric_knowledge_conflicts",
    "license": "Apache 2.0",
    "hash": "617ac994d1221faf"
  },
  "mmlu-anatomy": {
    "id": "mmlu-anatomy",
    "name": "MMLU Anatomy",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Anatomy",
    "examples": null,
    "num_of_dataset_prompts": 135,
    "created_date": "2025-05-19 14:29:53",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "acd80be5e24e9d97"
  },
  "singapore-transport-system": {
    "id": "singapore-transport-system",
    "name": "Singapore Transport System",
    "description": "Contain questions about Singapore's transport system",
    "examples": null,
    "num_of_dataset_prompts": 27,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "549a11727c31f84a"
  },
  "truthfulqa-multiple-open-ended": {
    "id": "truthfulqa-multiple-open-ended",
    "name": "truthfulqa",
    "description": "TruthfulQA aims to measure the the truthfulness of a model.",
    "examples": null,
    "num_of_dataset_prompts": 817,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "0d848befa72de159"
  },
  "arc-easy": {
    "id": "arc-easy",
    "name": "ARC-Easy",
    "description": "A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the easy set.",
    "examples": null,
    "num_of_dataset_prompts": 5197,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://allenai.org/data/arc",
    "license": "CC BY-SA",
    "hash": "9d3b5d92bde69a2d"
  },
  "cbbq-lite-ethnicity-disamb": {
    "id": "cbbq-lite-ethnicity-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 980,
    "created_date": "2025-05-19 18:07:03",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "b8675621af0eb685"
  },
  "singapore-iconic-places": {
    "id": "singapore-iconic-places",
    "name": "Iconic Places in Singapore",
    "description": "Contain questions about Singapore's iconic places.",
    "examples": null,
    "num_of_dataset_prompts": 16,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "4b261a3a7d15fca9"
  },
  "cbbq-lite-disability-ambiguous": {
    "id": "cbbq-lite-disability-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1588,
    "created_date": "2025-05-19 18:06:32",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "71d2b63d6a3b3a6e"
  },
  "cbbq-lite-SES-ambiguous": {
    "id": "cbbq-lite-SES-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 3960,
    "created_date": "2025-05-19 18:08:05",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "ff216840e0a7626e"
  },
  "mlc-scr-typical-user": {
    "id": "mlc-scr-typical-user",
    "name": "MLC Baseline - Sex-Related Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of sex-related crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:07:36",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "f9ab0240848b3b63"
  },
  "analogical-similarity": {
    "id": "analogical-similarity",
    "name": "Analogical Similarity",
    "description": "To measure the model’s ability in discriminating between different degrees of analogical similarity in two given episodes",
    "examples": null,
    "num_of_dataset_prompts": 323,
    "created_date": "2025-05-19 18:05:47",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/analogical_similarity",
    "license": "Apache 2.0",
    "hash": "897d11c6f74e806d"
  },
  "cbbq-lite-age-disamb": {
    "id": "cbbq-lite-age-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 7400,
    "created_date": "2025-05-19 18:06:29",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "4f4656aa00393667"
  },
  "bbq-lite-race-ethnicity-disamb": {
    "id": "bbq-lite-race-ethnicity-disamb",
    "name": "BBQ-lite on race-ethnicity - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 3440,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "6b56199f5a3cf947"
  },
  "gsm8k": {
    "id": "gsm8k",
    "name": "GSM8K",
    "description": "This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations.",
    "examples": null,
    "num_of_dataset_prompts": 8792,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/openai/grade-school-math",
    "license": "MIT License",
    "hash": "11ed0535647e924e"
  },
  "mlc-cae-malicious-user": {
    "id": "mlc-cae-malicious-user",
    "name": "MLC Baseline - Child Abuse and Exploitation",
    "description": "Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.",
    "examples": null,
    "num_of_dataset_prompts": 1330,
    "created_date": "2025-06-03 16:04:57",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "04241cd2207f15ef"
  },
  "bbq-lite-race-x-gender-disamb": {
    "id": "bbq-lite-race-x-gender-disamb",
    "name": "BBQ-lite on race-x-gender - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 7980,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c9ef01468e5fc7a5"
  },
  "mlc-scr-malicious-user": {
    "id": "mlc-scr-malicious-user",
    "name": "MLC Baseline - Sex-Related Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of sex-related crimes.",
    "examples": null,
    "num_of_dataset_prompts": 1330,
    "created_date": "2025-06-03 16:07:31",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "a9e75c21c7c8c8a4"
  },
  "bbq-lite-disability-status-ambiguous": {
    "id": "bbq-lite-disability-status-ambiguous",
    "name": "BBQ-lite on disability-status - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 778,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "6883fd4375aafc6b"
  },
  "mmlu-collegebiology": {
    "id": "mmlu-collegebiology",
    "name": "MMLU College Biology",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Biology",
    "examples": null,
    "num_of_dataset_prompts": 144,
    "created_date": "2025-05-19 14:30:04",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "c9038f4c75b0784d"
  },
  "challenging-toxicity-prompts-variation1": {
    "id": "challenging-toxicity-prompts-variation1",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "0ebc16658eda18be"
  },
  "bbq-lite-religion-disamb": {
    "id": "bbq-lite-religion-disamb",
    "name": "BBQ-lite on religion - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 600,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "e9bf212e60bf1875"
  },
  "mlc-cae-typical-user": {
    "id": "mlc-cae-typical-user",
    "name": "MLC Baseline - Child Abuse and Exploitation",
    "description": "Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:05:06",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "14235eea94053bec"
  },
  "cbbq-lite-nationality-disamb": {
    "id": "cbbq-lite-nationality-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 11988,
    "created_date": "2025-05-19 18:07:32",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "3dfa103cc6aaa58d"
  },
  "cbbq-lite-region-ambiguous": {
    "id": "cbbq-lite-region-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 2176,
    "created_date": "2025-05-19 18:07:50",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "206c1dfb98a3c8ea"
  },
  "cbbq-lite-household-registration-disamb": {
    "id": "cbbq-lite-household-registration-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 8700,
    "created_date": "2025-05-19 18:07:19",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "8bfaa27ccf6382e2"
  },
  "cbbq-lite-physical-appearance-ambiguous": {
    "id": "cbbq-lite-physical-appearance-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1856,
    "created_date": "2025-05-19 18:07:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "05918182f71a8748"
  },
  "mlc-nvc-typical-user": {
    "id": "mlc-nvc-typical-user",
    "name": "MLC Baseline - Non-Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:07:06",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "48b6cbfe00c57a81"
  },
  "mlc-nvc-malicious-user": {
    "id": "mlc-nvc-malicious-user",
    "name": "MLC Baseline - Non-Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 1810,
    "created_date": "2025-06-03 16:07:00",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "75b394ee8211c71f"
  },
  "occupation": {
    "id": "occupation",
    "name": "List of Occupations",
    "description": "This dataset contains a list of occupations collected from various websites (e.g., MoM of Singapore). The dataset can be used to test job-related applications.",
    "examples": null,
    "num_of_dataset_prompts": 7270,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "MIT License",
    "hash": "8b3a26c43b4e0eb1"
  },
  "jailbreak-dan": {
    "id": "jailbreak-dan",
    "name": "In-the-wild Jailbreak Dataset",
    "description": "This dataset contains jailbreak prompts that were collected from various places (e.g., Discord).",
    "examples": null,
    "num_of_dataset_prompts": 22,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/verazuo/jailbreak_llms",
    "license": "MIT license",
    "hash": "285e30a83660d6f8"
  },
  "mlc-ssh-vulnerable-user": {
    "id": "mlc-ssh-vulnerable-user",
    "name": "MLC Baseline - Suicide & Self-Harm",
    "description": "Responses that enable, encourage, or endorse acts of intentional self-harm.",
    "examples": null,
    "num_of_dataset_prompts": 1090,
    "created_date": "2025-06-03 16:08:04",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "6ed6ce481b77659d"
  },
  "mlc-vcr-typical-user": {
    "id": "mlc-vcr-typical-user",
    "name": "MLC Baseline - Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2025-06-03 16:08:57",
    "reference": "https://arxiv.org/html/2404.12241v1",
    "license": "CC-BY-4.0 license",
    "hash": "046978a237f453b8"
  },
  "commonsense-morality-hard-variation1": {
    "id": "commonsense-morality-hard-variation1",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "3e6db1a3ed29a606"
  },
  "auto-categorisation": {
    "id": "auto-categorisation",
    "name": "auto_categorization",
    "description": "To measure the model's ability in categorising elements into groups.",
    "examples": null,
    "num_of_dataset_prompts": 328,
    "created_date": "2025-05-19 18:05:57",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/auto_categorization",
    "license": "Apache 2.0",
    "hash": "fa24fa93bbdd7eed"
  },
  "bbq-lite-sexual-orientation-disamb": {
    "id": "bbq-lite-sexual-orientation-disamb",
    "name": "BBQ-lite on sexual-orientation - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "11de760afd6ac745"
  },
  "tamil-tamilnews-classification": {
    "id": "tamil-tamilnews-classification",
    "name": "tamil-news-classification",
    "description": "This dataset is used to classify a static set of Tamil News. The task is to classify news to its respective category. The dataset has 6 news categories - \"tamilnadu\", \"india\", \"cinema\", \"sports\", \"politics\", \"world\".",
    "examples": null,
    "num_of_dataset_prompts": 3631,
    "created_date": "2025-01-15 09:09:04",
    "reference": "https://github.com/vanangamudi/tamil-news-classification/tree/master/dataset/news",
    "license": "GNU General Public License v3.0",
    "hash": "255585f44726010d"
  },
  "cbbq-lite-sexual-orientation-disamb": {
    "id": "cbbq-lite-sexual-orientation-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 560,
    "created_date": "2025-05-19 18:08:16",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "18369fd300b31559"
  },
  "cvalues": {
    "id": "cvalues",
    "name": "C-Values - Values of Chinese Large Language attack_modules",
    "description": "This dataset contains prompts that can measure the alignment ability of LLMs in both safety and responsibility in Chinese Language.",
    "examples": null,
    "num_of_dataset_prompts": 1712,
    "created_date": "2025-01-15 09:08:50",
    "reference": "https://github.com/X-PLUG/CValues",
    "license": "Apache-2.0 license",
    "hash": "d3427b3869751e52"
  },
  "bbq-lite-physical-appearance-ambiguous": {
    "id": "bbq-lite-physical-appearance-ambiguous",
    "name": "BBQ-lite on physical-appearance - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 788,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "9438943944ee4535"
  },
  "privacy-enronemails": {
    "id": "privacy-enronemails",
    "name": "enronemail",
    "description": "The Enron email dataset is aimed at evaluating if there is leakage of private information. The dataset was used in DecodingTrust as part of its privacy evaluation.",
    "examples": null,
    "num_of_dataset_prompts": 166418,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/ParakweetLabs/EmailIntentDataSet",
    "license": "Apache-2.0",
    "hash": "afc1dd31ee16a005"
  },
  "mmlu-medicalgenetics": {
    "id": "mmlu-medicalgenetics",
    "name": "MMLU Medical Genetics",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Medical Genetics",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 14:30:18",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "1bdd3c199638358f"
  },
  "cbbq-lite-gender-ambiguous": {
    "id": "cbbq-lite-gender-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1534,
    "created_date": "2025-05-19 18:07:06",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "a6aa84d4fa221098"
  },
  "bbq-lite-ses-ambiguous": {
    "id": "bbq-lite-ses-ambiguous",
    "name": "BBQ-lite on ses - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 3432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c17e07c454222faf"
  },
  "truthfulqa-open-ended": {
    "id": "truthfulqa-open-ended",
    "name": "truthfulqa",
    "description": "TruthfulQA aims to measure the the truthfulness of a model.",
    "examples": null,
    "num_of_dataset_prompts": 817,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "852847eafe14fd57"
  },
  "commonsense-morality-easy-variation1": {
    "id": "commonsense-morality-easy-variation1",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "a8585d955f3d7855"
  },
  "singapore-places-tnf": {
    "id": "singapore-places-tnf",
    "name": "Places in Singapore",
    "description": "Contain prompts that test model's udnerstanding places in Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "525bac1813e88f2d"
  },
  "bbq-lite-nationality-disamb": {
    "id": "bbq-lite-nationality-disamb",
    "name": "BBQ-lite on nationality - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 1540,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "fccf218fba4ae20f"
  },
  "mmlu-professionalmedicine": {
    "id": "mmlu-professionalmedicine",
    "name": "MMLU Professional Medicine.json",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Professional Medicine.json",
    "examples": null,
    "num_of_dataset_prompts": 272,
    "created_date": "2025-05-19 14:30:25",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "713b6d53fc583e5b"
  },
  "cbbq-lite-disease-disamb": {
    "id": "cbbq-lite-disease-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 586,
    "created_date": "2025-05-19 18:06:50",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "00c14e175a680598"
  },
  "bbq-lite-race-ethnicity-ambiguous": {
    "id": "bbq-lite-race-ethnicity-ambiguous",
    "name": "BBQ-lite on race-ethnicity - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 3440,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "86420a8db0b2e672"
  },
  "cbbq-lite-disability-disamb": {
    "id": "cbbq-lite-disability-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1588,
    "created_date": "2025-05-19 18:06:37",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "71d2b63d6a3b3a6e"
  },
  "winobias-type1": {
    "id": "winobias-type1",
    "name": "winobias-variation1",
    "description": "This dataset contains gender-bias based on the professions from the Labor Force Statistics (https://www.bls.gov/cps/cpsaat11.htm), which contain some gender-bias.",
    "examples": null,
    "num_of_dataset_prompts": 396,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino",
    "license": "MIT License",
    "hash": "c7c7603cd4db3a8d"
  },
  "cbbq-lite-educational-qualification-ambiguous": {
    "id": "cbbq-lite-educational-qualification-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1392,
    "created_date": "2025-05-19 18:06:53",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "a558488c7964395e"
  },
  "cbbq-lite-race-ambiguous": {
    "id": "cbbq-lite-race-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2025-05-19 18:07:41",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "6c7b712114eac898"
  },
  "hellaswag": {
    "id": "hellaswag",
    "name": "HellaSwag",
    "description": "This dataset is used to evaluate commonsense with questions that are trivial for humans but difficult for state-of-the-art models.",
    "examples": null,
    "num_of_dataset_prompts": 49947,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/rowanz/hellaswag",
    "license": "MIT License",
    "hash": "eed60f3a88abe77b"
  },
  "bbq-lite-ses-disamb": {
    "id": "bbq-lite-ses-disamb",
    "name": "BBQ-lite on ses - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 3432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "390179387b6961b0"
  },
  "singapore-safety-questions": {
    "id": "singapore-safety-questions",
    "name": "Safety Benchmark (Singapore Context)",
    "description": "Contain prompts that test safety in Singapore-context",
    "examples": null,
    "num_of_dataset_prompts": 59,
    "created_date": "2024-10-15 21:25:44",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "984714efbead7cc1"
  },
  "medqa-us": {
    "id": "medqa-us",
    "name": "MedQA (US)",
    "description": "MedQA is a free-form multiple-choice OpenQA dataset for solving medical problems. These are collected from the professional medical board exams. We extracted the list of MCQ questions from the US in this dataset.",
    "examples": null,
    "num_of_dataset_prompts": 10178,
    "created_date": "2025-05-19 14:27:15",
    "reference": "https://github.com/jind11/MedQA",
    "license": "MIT License",
    "hash": "4fd3fd50fab0560b"
  },
  "uciadult": {
    "id": "uciadult",
    "name": "uciadult",
    "description": "The UCI adult dataset, created in 1996, is used to train models to predict whether a person's income will exceed $50K/yr based on census data. Also known as \"Census Income\" dataset. ",
    "examples": null,
    "num_of_dataset_prompts": 32561,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://archive.ics.uci.edu/dataset/2/adult",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "bc4b1b4d64166777"
  },
  "mmlu-all": {
    "id": "mmlu-all",
    "name": "MMLU",
    "description": "This dataset covers 57 tasks including elementary mathemathics, US history, computer science, law, and more.",
    "examples": null,
    "num_of_dataset_prompts": 17487,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT license",
    "hash": "67315ab3ec7e236e"
  },
  "cbbq-lite-sexual-orientation-ambiguous": {
    "id": "cbbq-lite-sexual-orientation-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 560,
    "created_date": "2025-05-19 18:08:14",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "18369fd300b31559"
  },
  "cbbq-lite-ethnicity-ambiguous": {
    "id": "cbbq-lite-ethnicity-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 980,
    "created_date": "2025-05-19 18:07:00",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "CC-BY-SA-4.0 license",
    "hash": "b8675621af0eb685"
  },
  "bbq-lite-disability-status-disamb": {
    "id": "bbq-lite-disability-status-disamb",
    "name": "BBQ-lite on disability-status - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 778,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "1645815ffe8f413f"
  },
  "m3exam-vietnamese-test": {
    "id": "m3exam-vietnamese-test",
    "name": "m3exam-vietnamese",
    "description": "This dataset contains local exam questions for testing Vietnamese capability.",
    "examples": null,
    "num_of_dataset_prompts": 1745,
    "created_date": "2024-05-29 18:16:14",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "27805df65119abe7"
  },
  "m3exam-indonesian-test": {
    "id": "m3exam-indonesian-test",
    "name": "m3exam-indonesian",
    "description": "This dataset contains local exam questions for testing Indonesian capability.",
    "examples": null,
    "num_of_dataset_prompts": 1580,
    "created_date": "2024-05-29 18:16:14",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "d73b6facd5105e3e"
  },
  "m3exam-thai-test": {
    "id": "m3exam-thai-test",
    "name": "m3exam-thai",
    "description": "This dataset contains local exam questions for testing Thai capability.",
    "examples": null,
    "num_of_dataset_prompts": 2126,
    "created_date": "2024-05-29 18:16:14",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "90f3ed81ff2cdab9"
  },
  "clcc": {
    "id": "clcc",
    "name": "Chinese Linguistics & Cognition Challenge (CLCC)",
    "description": "This dataset is a subjective benchmark created by BAAI FlagEval group.",
    "examples": null,
    "num_of_dataset_prompts": 760,
    "created_date": "2024-06-22 13:04:43",
    "reference": "https://flageval.baai.ac.cn/",
    "license": "CC-BY-4.0 license",
    "hash": "2b1fc98e8e99db78"
  },
  "i2p-text2image-prompts": {
    "id": "i2p-text2image-prompts",
    "name": "Inappropriate Image Prompts (I2P)",
    "description": "The I2P benchmark contains real user prompts for generative text2image prompts that are unproportionately likely to produce inappropriate images. I2P was introduced in the 2023 CVPR paper Safe Latent Diffusion: Mitigating Inappropriate Degeneration in Diffusion Models. This benchmark is not specific to any approach or model, but was designed to evaluate mitigating measures against inappropriate degeneration in Stable Diffusion.",
    "examples": null,
    "num_of_dataset_prompts": 4703,
    "created_date": "2024-08-08 08:29:13",
    "reference": "https://huggingface.co/datasets/AIML-TUDA/i2p",
    "license": "MIT License",
    "hash": "c169a479266a5e41"
  },
  "gender-occupation-text2-image-prompts": {
    "id": "gender-occupation-text2-image-prompts",
    "name": "Gender Occupational Bias",
    "description": "The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759",
    "examples": null,
    "num_of_dataset_prompts": 13,
    "created_date": "2024-08-08 20:49:11",
    "reference": "https://arxiv.org/abs/2211.03759",
    "license": "MIT License",
    "hash": "8b0a309e8101a90a"
  },
  "gender-text2-image-prompts": {
    "id": "gender-text2-image-prompts",
    "name": "Gender Occupational Bias",
    "description": "The gender occupational bias is a set of gender neutral text-to-image prompts that are likely to result in models favouring the generation of one gender over the other. The occupations included were proposed in the paper: https://arxiv.org/abs/2211.03759",
    "examples": null,
    "num_of_dataset_prompts": 13,
    "created_date": "2024-08-08 20:49:11",
    "reference": "https://arxiv.org/abs/2211.03759",
    "license": "MIT License",
    "hash": "8b0a309e8101a90a"
  },
  "cyberseceval_promptinjection": {
    "id": "cyberseceval_promptinjection",
    "name": "CyberSecEval Prompt Injection",
    "description": "Check if llm is susceptible to prompt injection",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2024-10-03 16:46:16",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "64c43fa0d20dbc53"
  },
  "facticity-apac-multilungual-facts": {
    "id": "facticity-apac-multilungual-facts",
    "name": "Facts about Asia pacific in True and False in 4 languages (Chinese, Malay, Tamil and English)",
    "description": "Contain prompts that contain facts about Asia Pacific and Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 22,
    "created_date": "2024-10-03 16:46:16",
    "reference": "facticity.ai",
    "license": "Apache-2.0",
    "hash": "5a5c4d22e8266c20"
  },
  "squad-v2": {
    "id": "squad-v2",
    "name": "squad-v2",
    "description": "Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage, or the question might be unanswerable.",
    "examples": null,
    "num_of_dataset_prompts": 11873,
    "created_date": "2025-01-06 01:02:59",
    "reference": "rajpurkar/squad_v2 · Datasets at Hugging Face",
    "license": "CC BY-SA 4.0",
    "hash": "0ce71fdd1eb01e07"
  },
  "gsm8k-test": {
    "id": "gsm8k-test",
    "name": "GSM8K-TEST",
    "description": "This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations. The dataset includes only the test set of GSM8K.",
    "examples": null,
    "num_of_dataset_prompts": 1319,
    "created_date": "2025-01-06 01:02:59",
    "reference": "https://github.com/openai/grade-school-math",
    "license": "MIT License",
    "hash": "e52644f58e4cee5f"
  },
  "answercarefully-kr": {
    "id": "answercarefully-kr",
    "name": "Answercarefully Information Korean",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 10:28:00",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "6b035d502977c581"
  },
  "cyberseceval-promptinjection2-en": {
    "id": "cyberseceval-promptinjection2-en",
    "name": "CyberSecEval Prompt Injection 2 English",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 11:26:47",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "bed7aa704abd13d9"
  },
  "mlc-ipv-en": {
    "id": "mlc-ipv-en",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - English",
    "description": "Test whether responses violate the intellectual property rights of any party.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:40:50",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "f0d825ba663eab9a"
  },
  "answercarefully-ch": {
    "id": "answercarefully-ch",
    "name": "Answercarefully Information Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 10:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "e252ee595a66daa7"
  },
  "answercarefully-en": {
    "id": "answercarefully-en",
    "name": "Answercarefully Information English",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-07 10:27:18",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "4eba4b01576a3133"
  },
  "answercarefully-jp": {
    "id": "answercarefully-jp",
    "name": "Answercarefully Information Japanese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-07 10:27:43",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "b634d3df8f624da8"
  },
  "mlc-ncr-en": {
    "id": "mlc-ncr-en",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - English",
    "description": "Test whether responses enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "22de95ee4fc3c83f"
  },
  "mlc-prv-en": {
    "id": "mlc-prv-en",
    "name": "ML Commons  Ailuminte - Privacy - English",
    "description": "Test whether responses contain sensitive, nonpublic personal information that could undermine someone’s physical, digital, or financial security.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "aafa5277cf9e1332"
  },
  "mlc-vcr-en": {
    "id": "mlc-vcr-en",
    "name": "ML Commons Ailuminte - Violent Crimes - English",
    "description": "Test whether responses enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "1ee36553e06737a7"
  },
  "mlc-vcr-kw": {
    "id": "mlc-vcr-kw",
    "name": "ML Commons Ailuminte - Violent Crimes - Kwashili",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "984644bbd589623a"
  },
  "mlc-ipv-my": {
    "id": "mlc-ipv-my",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Malay",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:57",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "da92d1c23ce22cf5"
  },
  "mlc-vcr-fr": {
    "id": "mlc-vcr-fr",
    "name": "ML Commons Ailuminte - Violent Crimes - French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "f6215ceed24c8680"
  },
  "mlc-prv-fr": {
    "id": "mlc-prv-fr",
    "name": "ML Commons Ailuminte - Privacy - Farsi",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "541b9a46f34f61ff"
  },
  "mlc-ncr-fr": {
    "id": "mlc-ncr-fr",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - French",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "72e536f6b2566cc5"
  },
  "mlc-prv-kw": {
    "id": "mlc-prv-kw",
    "name": "ML Commons Ailuminte - Privacy - Kwashili",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "ed0e841e5cf7b393"
  },
  "mlc-ncr-kw": {
    "id": "mlc-ncr-kw",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Kwashili",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "a99121bedf931d75"
  },
  "mlc-ipv-tl": {
    "id": "mlc-ipv-tl",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Telugu",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "ca31eac9bafee128"
  },
  "mlc-prv-jp": {
    "id": "mlc-prv-jp",
    "name": "ML Commons Ailuminte - Privacy - Japanese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 16:19:51",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "9f81c61eb0527047"
  },
  "mlc-ncr-jp": {
    "id": "mlc-ncr-jp",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Japanese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "369bd90d74649c2a"
  },
  "mlc-ipv-cn": {
    "id": "mlc-ipv-cn",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:13",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "1bd8b18a3eedaf56"
  },
  "mlc-vcr-jp": {
    "id": "mlc-vcr-jp",
    "name": "ML Commons Ailuminte - Violent Crimes - Japanese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "b9089866e7e4dfd1"
  },
  "answercarefully-tl": {
    "id": "answercarefully-tl",
    "name": "Answercarefully Information Telugu",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "1f8944744320293e"
  },
  "mlc-ncr-fa": {
    "id": "mlc-ncr-fa",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Farsi",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "01c0f232ab7ec952"
  },
  "mlc-ipv-kr": {
    "id": "mlc-ipv-kr",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Korean",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:45",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "97973fef96712d92"
  },
  "answercarefully-my": {
    "id": "answercarefully-my",
    "name": "Answercarefully Information Malay",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "c5a6da2155a853ca"
  },
  "cyberseceval-promptinjection2-kr": {
    "id": "cyberseceval-promptinjection2-kr",
    "name": "CyberSecEval Prompt Injection 2 Korean",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 11:26:47",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "74f46da151d8c162"
  },
  "answercarefully-cn": {
    "id": "answercarefully-cn",
    "name": "Answercarefully Information Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 56,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing",
    "hash": "e252ee595a66daa7"
  },
  "answercarefully-ca": {
    "id": "answercarefully-ca",
    "name": "Answercarefully Information Cantonese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "02cd5cb174e7066f"
  },
  "mlc-ipv-fa": {
    "id": "mlc-ipv-fa",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Farsi",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:31",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "bafd6a99778a1c4c"
  },
  "answercarefully-kw": {
    "id": "answercarefully-kw",
    "name": "Answercarefully Information Kwashili",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "9d0034bbf4af2125"
  },
  "mlc-vcr-kr": {
    "id": "mlc-vcr-kr",
    "name": "ML Commons Ailuminte - Violent Crimes - Korean",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "35f3d72bcdde7376"
  },
  "answercarefully-fr": {
    "id": "answercarefully-fr",
    "name": "Answercarefully Information French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "dd56c14bd30171a0"
  },
  "prompt_injection_jailbreak": {
    "id": "prompt_injection_jailbreak",
    "name": "Jailbreak Prompts",
    "description": "This dataset evaluates whether a model/app will detect attempted jailbreaks and generate unsafe content. These jailbreak techniques are collected from X (e.g., Pliny) and public repository (e.g., in-the-wild jailbreak prompts). ",
    "examples": null,
    "num_of_dataset_prompts": 3600,
    "created_date": "2025-01-08 11:26:33",
    "reference": "OWASP",
    "license": "Apache 2.0",
    "hash": "c5720f9447be93c3"
  },
  "mlc-prv-kr": {
    "id": "mlc-prv-kr",
    "name": "ML Commons Ailuminte - Privacy - Korean",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "1c77a704a0a2328b"
  },
  "mlc-ncr-kr": {
    "id": "mlc-ncr-kr",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Korean",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "6ef3e555345de60b"
  },
  "mlc-prv-cn": {
    "id": "mlc-prv-cn",
    "name": "ML Commons Ailuminte - Privacy - Chinese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "b871c59b1c2d2b11"
  },
  "cyberseceval-promptinjection2-jp": {
    "id": "cyberseceval-promptinjection2-jp",
    "name": "CyberSecEval Prompt Injection 2 Japanese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 11:26:47",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "e37764350ce4417a"
  },
  "mlc-ncr-cn": {
    "id": "mlc-ncr-cn",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "5d045a277cd36184"
  },
  "mlc-ipv-jp": {
    "id": "mlc-ipv-jp",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Japanese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:41",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "24126e63f17269f6"
  },
  "mlc-ipv-ca": {
    "id": "mlc-ipv-ca",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Cantonese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "e8239366599b7bfb"
  },
  "mlc-vcr-cn": {
    "id": "mlc-vcr-cn",
    "name": "ML Commons Ailuminte - Violent Crimes - Chinese",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "d5cc96f249163b05"
  },
  "mlc-prv-my": {
    "id": "mlc-prv-my",
    "name": "ML Commons Ailuminte - Privacy - Malay",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "30e4065bc9ef0e95"
  },
  "mlc-ncr-my": {
    "id": "mlc-ncr-my",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Malay",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "f120562bd1f9d5ae"
  },
  "mlc-vcr-tl": {
    "id": "mlc-vcr-tl",
    "name": "ML Commons Ailuminte - Violent Crimes - Telugu",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "157836d903a26af5"
  },
  "mlc-prv-tl": {
    "id": "mlc-prv-tl",
    "name": "ML Commons Ailuminte - Privacy - Telugu",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "7942a37e21ce5594"
  },
  "mlc-ipv-kw": {
    "id": "mlc-ipv-kw",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - Kwashili",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:50",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "7a9b83b4e044abbc"
  },
  "answercarefully-fa": {
    "id": "answercarefully-fa",
    "name": "Answercarefully Information Farsi",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 48,
    "created_date": "2025-01-08 11:26:47",
    "reference": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "license": "Dataset from NII-LLMC working group - subset created for AISI testing.",
    "hash": "89de87d80a59a150"
  },
  "mlc-ncr-tl": {
    "id": "mlc-ncr-tl",
    "name": "ML Commons Ailuminte - Non-Violent Crimes - Telugu",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "aac2c831fc81d134"
  },
  "cyberseceval-promptinjection2-kw": {
    "id": "cyberseceval-promptinjection2-kw",
    "name": "CyberSecEval Prompt Injection 2 Kwashili",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 11:26:47",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "f31a5d48761cb690"
  },
  "mlc-ipv-fr": {
    "id": "mlc-ipv-fr",
    "name": "ML Commons Ailuminte - Intellectual Property Violation - French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:30:37",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "7ff300e33e70c091"
  },
  "mlc-vcr-my": {
    "id": "mlc-vcr-my",
    "name": "ML Commons Ailuminte - Violent Crimes - Malay",
    "description": "This recipe measures the model's responses in dealing with prompts that demean or dehumanize people on the basis of their sensitive, personal characteristics. A high grade indicates that the system under test handles the response well.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:32:04",
    "reference": "https://mlcommons.org/ailuminate/",
    "license": "CC-BY-4.0 license",
    "hash": "cd839fe62984cd08"
  },
  "cyberseceval-promptinjection2-my": {
    "id": "cyberseceval-promptinjection2-my",
    "name": "CyberSecEval Prompt Injection 2 Malay",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 17:09:06",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "dd64577f5a5cb1bf"
  },
  "cyberseceval-promptinjection2-cn": {
    "id": "cyberseceval-promptinjection2-cn",
    "name": "CyberSecEval Prompt Injection 2 Chinese",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 17:09:06",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "0eff135215437449"
  },
  "cyberseceval-promptinjection2-fr": {
    "id": "cyberseceval-promptinjection2-fr",
    "name": "CyberSecEval Prompt Injection 2 French",
    "description": "A dataset of security-related questions and answers.",
    "examples": null,
    "num_of_dataset_prompts": 251,
    "created_date": "2025-01-08 17:09:06",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks/datasets/prompt_injection",
    "license": "MIT license",
    "hash": "5e588e9955be5957"
  },
  "rag-sample-dataset": {
    "id": "rag-sample-dataset",
    "name": "mock-dataset",
    "description": "This is a sample of a dataset to be used for RAG Evaluation. ",
    "examples": null,
    "num_of_dataset_prompts": 3,
    "created_date": "2025-05-02 11:34:21",
    "reference": "",
    "license": "",
    "hash": "b6d17027d92b8e22"
  },
  "singapore-pofma-statements-2024": {
    "id": "singapore-pofma-statements-2024",
    "name": "Singapore POMFA Statements",
    "description": "Statements that are false under POFMA in Singapore for 2024.",
    "examples": null,
    "num_of_dataset_prompts": 36,
    "created_date": "2025-05-14 01:04:27",
    "reference": "Genue",
    "license": "Apache-2.0",
    "hash": "6767c034139ae3b0"
  },
  "singapore-pofma-statements-2023": {
    "id": "singapore-pofma-statements-2023",
    "name": "Singapore POMFA Statements",
    "description": "Statements that are false under POFMA in Singapore for 2023.",
    "examples": null,
    "num_of_dataset_prompts": 34,
    "created_date": "2025-05-14 01:04:27",
    "reference": "Genue",
    "license": "Apache-2.0",
    "hash": "21cf49c5b0c61e2c"
  },
  "singapore-pofma-true-statements": {
    "id": "singapore-pofma-true-statements",
    "name": "Singapore POMFA True Statements",
    "description": "Statements that are true under POFMA in Singapore. For sanity check only.",
    "examples": null,
    "num_of_dataset_prompts": 14,
    "created_date": "2025-05-14 01:04:27",
    "reference": "Genue",
    "license": "Apache-2.0",
    "hash": "eb114acc321084b2"
  },
  "bipia-abstract-test": {
    "id": "bipia-abstract-test",
    "name": "BIPIA - abstract QA - English",
    "description": "Abstrct QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. Based on XSum dataset (BBC articles). Fake summaries or call-to-action embedded in articles.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-16 15:58:43",
    "reference": "https://github.com/microsoft/BIPIA/tree/main",
    "license": "MIT License",
    "hash": "5508d132a0697055"
  },
  "bipia-abstract-train": {
    "id": "bipia-abstract-train",
    "name": "BIPIA - abstract QA - English",
    "description": "Abstrct QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. Based on XSum dataset (BBC articles). Fake summaries or call-to-action embedded in articles.",
    "examples": null,
    "num_of_dataset_prompts": 900,
    "created_date": "2025-05-16 15:58:55",
    "reference": "https://github.com/microsoft/BIPIA/tree/main",
    "license": "MIT License",
    "hash": "288a906446cbd4b6"
  },
  "bipia-email-test": {
    "id": "bipia-email-test",
    "name": "BIPIA - email QA - English",
    "description": "Email QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. Based on OpenAI Evals (real-world emails with questions and answers). Malicious instructions may be hidden in the email body.",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2025-05-16 15:58:59",
    "reference": "https://github.com/microsoft/BIPIA/tree/main",
    "license": "MIT License",
    "hash": "8eee39330d130787"
  },
  "bipia-email-train": {
    "id": "bipia-email-train",
    "name": "BIPIA - email QA - English",
    "description": "Email QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. Based on OpenAI Evals (real-world emails with questions and answers). Malicious instructions may be hidden in the email body.",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2025-05-16 15:59:05",
    "reference": "https://github.com/microsoft/BIPIA/tree/main",
    "license": "MIT License",
    "hash": "f8ac19ee76f6ecc9"
  },
  "bipia-news": {
    "id": "bipia-news",
    "name": "BIPIA - News QA - English",
    "description": "News QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. News data is based on NewsQA dataset. Malicious content injected into search snippets or web pages.",
    "examples": null,
    "num_of_dataset_prompts": 5166,
    "created_date": "2025-05-15 14:59:29",
    "reference": "https://www.kaggle.com/datasets/nagendra048/newsqa-dataset, https://github.com/microsoft/BIPIA/tree/main",
    "license": " MIT License",
    "hash": "bfd45e70d532d32c"
  },
  "bipia-table-test": {
    "id": "bipia-table-test",
    "name": "BIPIA - table QA - English",
    "description": "Table QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. Based on WikiTableQuestions dataset. Attacks embedded as cell notes or footnotes.",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 15:39:09",
    "reference": "https://github.com/microsoft/BIPIA/tree/main",
    "license": "MIT license",
    "hash": "f5b174cf07fe0159"
  },
  "bipia-table-train": {
    "id": "bipia-table-train",
    "name": "BIPIA - table QA - English",
    "description": "Table QA from paper - Benchmarking and Defending Against Indirect Prompt Injection Attacks on Large Language Models. Based on WikiTableQuestions dataset.Attacks embedded as cell notes or footnotes.",
    "examples": null,
    "num_of_dataset_prompts": 900,
    "created_date": "2025-05-16 15:49:44",
    "reference": "https://github.com/microsoft/BIPIA/tree/main",
    "license": " MIT License",
    "hash": "38fd959f56984eba"
  },
  "cnn-dailymail": {
    "id": "cnn-dailymail",
    "name": "CNN/DailyMail",
    "description": "The CNN / DailyMail Dataset is an English-language dataset containing just over 300k unique news articles as written by journalists at CNN and the Daily Mail. This dataset contains the test split.",
    "examples": null,
    "num_of_dataset_prompts": 11490,
    "created_date": "2025-05-15 14:59:29",
    "reference": "https://huggingface.co/datasets/abisee/cnn_dailymail",
    "license": "Apache 2.0",
    "hash": "e6611a695472a45b"
  },
  "cyberseceval3-interpreter": {
    "id": "cyberseceval3-interpreter",
    "name": "CyberSecEval 3 Interpreter - English",
    "description": "These tests evaluate the security risks posed by integrating LLMs with code interpreters, specifically assessing how effectively an LLM can prevent malicious attempts to exploit the system or execute harmful code.",
    "examples": null,
    "num_of_dataset_prompts": 500,
    "created_date": "2025-05-19 13:44:32",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks",
    "license": "MIT License",
    "hash": "0097413911e54eb3"
  },
  "cyberseceval3-interpreter-2-modified": {
    "id": "cyberseceval3-interpreter-2-modified",
    "name": "CyberSecEval 3 Interpreter - English",
    "description": "These tests evaluate the security risks posed by integrating LLMs with code interpreters, specifically assessing how effectively an LLM can prevent malicious attempts to exploit the system or execute harmful code.",
    "examples": null,
    "num_of_dataset_prompts": 500,
    "created_date": "2025-05-15 16:00:52",
    "reference": "https://github.com/meta-llama/PurpleLlama/tree/main/CybersecurityBenchmarks",
    "license": "LLAMA 3.2 COMMUNITY LICENSE AGREEMENT",
    "hash": "feafc61bd4f1ecbf"
  },
  "decodingtrust-private-conversations": {
    "id": "decodingtrust-private-conversations",
    "name": "DecodingTrust - Privacy Leakage during Conversations",
    "description": "To test if applications can leak privacy-sensitive information which is provided during interactive conversations in the inference stage.",
    "examples": null,
    "num_of_dataset_prompts": 800,
    "created_date": "2025-05-19 15:18:57",
    "reference": "https://github.com/AI-secure/DecodingTrust/tree/main",
    "license": "CC BY-SA 4.0 license",
    "hash": "d4104eff4b274f0b"
  },
  "sghatecheck-ss": {
    "id": "sghatecheck-ss",
    "name": "SGHateCheck - Singlish",
    "description": "SGHateCheck is a hate speech benchmark tailored for Singapore's socio-linguistical context.",
    "examples": null,
    "num_of_dataset_prompts": 7023,
    "created_date": "2025-05-19 15:20:31",
    "reference": "https://github.com/Social-AI-Studio/SGHateCheck/tree/main/testcases",
    "license": "MIT license",
    "hash": "b08dea76fb97e900"
  },
  "sghatecheck-zh": {
    "id": "sghatecheck-zh",
    "name": "SGHateCheck - Chinese",
    "description": "SGHateCheck is a hate speech benchmark tailored for Singapore's socio-linguistical context.",
    "examples": null,
    "num_of_dataset_prompts": 5911,
    "created_date": "2025-05-19 15:20:38",
    "reference": "https://github.com/Social-AI-Studio/SGHateCheck/tree/main/testcases",
    "license": "MIT license",
    "hash": "9f9a72f5bb1fa098"
  },
  "sghatecheck-ta": {
    "id": "sghatecheck-ta",
    "name": "SGHateCheck - Chinese",
    "description": "SGHateCheck is a hate speech benchmark tailored for Singapore's socio-linguistical context.",
    "examples": null,
    "num_of_dataset_prompts": 2840,
    "created_date": "2025-05-19 15:20:35",
    "reference": "https://github.com/Social-AI-Studio/SGHateCheck/tree/main/testcases",
    "license": "MIT license",
    "hash": "489067092a6f5410"
  },
  "sghatecheck-ms": {
    "id": "sghatecheck-ms",
    "name": "SGHateCheck - Chinese",
    "description": "SGHateCheck is a hate speech benchmark tailored for Singapore's socio-linguistical context.",
    "examples": null,
    "num_of_dataset_prompts": 5911,
    "created_date": "2025-05-19 15:18:42",
    "reference": "https://github.com/Social-AI-Studio/SGHateCheck/tree/main/testcases",
    "license": "MIT license",
    "hash": "ee671ca81f831355"
  },
  "xstest": {
    "id": "xstest",
    "name": "XSTest",
    "description": "XSTest test suite highlights systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models",
    "examples": null,
    "num_of_dataset_prompts": 450,
    "created_date": "2025-05-19 13:56:38",
    "reference": "https://huggingface.co/datasets/walledai/XSTest",
    "license": "CC-BY-4.0 license",
    "hash": "1a61e53cbf4b5fae"
  },
  "xstest-privacy-subset": {
    "id": "xstest-privacy-subset",
    "name": "XSTest (Privacy related only)",
    "description": "XSTest test suite highlights systematic failure modes in state-of-the-art language models as well as more general challenges in building safer language models",
    "examples": null,
    "num_of_dataset_prompts": 75,
    "created_date": "2025-05-16 15:56:59",
    "reference": "https://huggingface.co/datasets/walledai/XSTest",
    "license": "Creative Commons Attribution 4.0 International license",
    "hash": "2239d08aedf20be6"
  },
  "mlc-ailuminate-src": {
    "id": "mlc-ailuminate-src",
    "name": "MLCAiluminate - Sex-related Crimes",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:55:15",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "a20e7d9b5077a3f8"
  },
  "mlc-ailuminate-iwp": {
    "id": "mlc-ailuminate-iwp",
    "name": "MLCAiluminate - Indiscriminate Weapons (CBRNE)",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:42",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "12883e9b35429def"
  },
  "safetybench-privacy-full": {
    "id": "safetybench-privacy-full",
    "name": "SafetyBench-PII (Test)",
    "description": "SafetyBench is a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns.",
    "examples": null,
    "num_of_dataset_prompts": 1299,
    "created_date": "2025-05-19 13:53:04",
    "reference": "https://huggingface.co/datasets/thu-coai/SafetyBench",
    "license": "MIT License",
    "hash": "da36f5d22b1f3044"
  },
  "safetybench-privacy-small": {
    "id": "safetybench-privacy-small",
    "name": "SafetyBench-PII (dev)",
    "description": "SafetyBench is a comprehensive benchmark for evaluating the safety of LLMs, which comprises 11,435 diverse multiple choice questions spanning across 7 distinct categories of safety concerns.",
    "examples": null,
    "num_of_dataset_prompts": 5,
    "created_date": "2025-05-19 18:04:10",
    "reference": "https://huggingface.co/datasets/thu-coai/SafetyBench",
    "license": "MIT license",
    "hash": "d894d1e8b19339af"
  },
  "mlc-ailuminate-spc-fin": {
    "id": "mlc-ailuminate-spc-fin",
    "name": "MLCAiluminate - Specialized Advice (Financial)",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 26,
    "created_date": "2025-05-19 13:55:01",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "dc1b69c95e5777e5"
  },
  "facts-grounding": {
    "id": "facts-grounding",
    "name": "FACTS Grounding",
    "description": "FACTS Grounding is a benchmark from Google DeepMind and Google Research designed to measure the performance of AI Models on factuality and grounding",
    "examples": null,
    "num_of_dataset_prompts": 860,
    "created_date": "2025-05-19 15:29:53",
    "reference": "https://huggingface.co/datasets/google/FACTS-grounding-public",
    "license": "CC-by-4.0 license",
    "hash": "0ef85cddbe99578d"
  },
  "mlc-ailuminate-ipv": {
    "id": "mlc-ailuminate-ipv",
    "name": "MLCAiluminate - Intellectual Property Violations",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:36",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "2f69ed3baabc4e4e"
  },
  "mlc-ailuminate-ssh": {
    "id": "mlc-ailuminate-ssh",
    "name": "MLCAiluminate - Suicide and Self-Harm",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:55:19",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "8900a0deed69c716"
  },
  "mlc-ailuminate-spc-lgl": {
    "id": "mlc-ailuminate-spc-lgl",
    "name": "MLCAiluminate - Specialized Advice (Legal)",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 24,
    "created_date": "2025-05-19 13:55:11",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "364570803eefbc49"
  },
  "mlc-ailuminate-cse": {
    "id": "mlc-ailuminate-cse",
    "name": "MLCAiluminate - Child Sexual Exploitation",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:08",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "c242fbbfba8d788f"
  },
  "mlc-ailuminate-spc-hlt": {
    "id": "mlc-ailuminate-spc-hlt",
    "name": "MLCAiluminate - Specialized Advice (Elections)",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 26,
    "created_date": "2025-05-19 13:55:05",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "f92bcaf0056df33b"
  },
  "mlc-ailuminate-spc-ele": {
    "id": "mlc-ailuminate-spc-ele",
    "name": "MLCAiluminate - Specialized Advice (Health)",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 24,
    "created_date": "2025-05-19 13:54:55",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "a4b8ce59ce102180"
  },
  "mlc-ailuminate-prv": {
    "id": "mlc-ailuminate-prv",
    "name": "MLCAiluminate - Privacy Violations",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:51",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "8f0fd0f6c8bc3685"
  },
  "mlc-ailuminate-ncr": {
    "id": "mlc-ailuminate-ncr",
    "name": "MLCAiluminate - Non-violent Crimes",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:46",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "870c16331d9061a1"
  },
  "mlc-ailuminate-vcr": {
    "id": "mlc-ailuminate-vcr",
    "name": "MLCAiluminate - Violent Crimes",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:55:29",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "0b5c10e97de12674"
  },
  "mlc-ailuminate-dfm": {
    "id": "mlc-ailuminate-dfm",
    "name": "MLCAiluminate - Defamation",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:18",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "db3a0b3a3c1e90fb"
  },
  "mlc-ailuminate-hte": {
    "id": "mlc-ailuminate-hte",
    "name": "MLCAiluminate - Hate",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:54:23",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "df4e4df72753aec2"
  },
  "mlc-ailuminate-sxc-prn": {
    "id": "mlc-ailuminate-sxc-prn",
    "name": "MLCAiluminate - Sexual Content (Pornographic)",
    "description": "The MLCommons AILuminate v1.0 benchmark provides safety testing for general purpose chat systems across 12 hazard categories",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2025-05-19 13:55:23",
    "reference": "https://github.com/mlcommons/ailuminate",
    "license": "CC-BY-4.0 license",
    "hash": "46f74a25053d45de"
  }
}