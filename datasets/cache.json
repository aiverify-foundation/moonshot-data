{
  "squad-shifts-tnf": {
    "id": "squad-shifts-tnf",
    "name": "squad-shifts-tnf",
    "description": "Zero-shot reading comprehension on paragraphs and questions from squadshifts",
    "examples": null,
    "num_of_dataset_prompts": 48201,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/squad_shifts",
    "license": "",
    "hash": "37b3413b520f1426"
  },
  "bbq-lite-religion-ambiguous": {
    "id": "bbq-lite-religion-ambiguous",
    "name": "BBQ-lite on religion - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 600,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "d2a0f4023c20d4fd"
  },
  "advglue-all": {
    "id": "advglue-all",
    "name": "advglue",
    "description": "Adversarial GLUE Benchmark (AdvGLUE) is a comprehensive robustness evaluation benchmark that focuses on the adversarial robustness evaluation of language models. ",
    "examples": null,
    "num_of_dataset_prompts": 721,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/adversarial-glue",
    "license": "",
    "hash": "93cd563eac992f8a"
  },
  "singapore-food-tnf": {
    "id": "singapore-food-tnf",
    "name": "Food in Singapore",
    "description": "Contain prompts that test model's udnerstanding in Food, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "675fc75234a5932a"
  },
  "medmcqa": {
    "id": "medmcqa",
    "name": "MedMCQA",
    "description": "MedMCQ is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrnace exam MCQs covering 2.4k healthcare topics and 21 medical subjects.",
    "examples": null,
    "num_of_dataset_prompts": 182822,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/MedMCQA/MedMCQA",
    "license": "MIT License",
    "hash": "53b7f1fd01b3c166"
  },
  "tamil-kural-classification": {
    "id": "tamil-kural-classification",
    "name": "tamil-thirukural",
    "description": "This dataset is used to test the comprehension abilities for the Thirukkural. Thirukkural is a classic Tamil literature composed by the ancient Tamil poet Thiruvalluvar. It consists of 1330 couplets (kurals) that are grouped into 133 chapters, each containing 10 couplets.",
    "examples": null,
    "num_of_dataset_prompts": 266,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/vijayanandrp/Thirukkural-Tamil-Dataset",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "52589869fc432c79"
  },
  "realtimeqa-past": {
    "id": "realtimeqa-past",
    "name": "RealtimeQA",
    "description": "RealTime QA is a dynamic question answering (QA) platform that inquires about the present. ",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/realtimeqa/realtimeqa_public",
    "license": "",
    "hash": "d4295a228fdf9a15"
  },
  "sg-university-tutorial-questions-legal": {
    "id": "sg-university-tutorial-questions-legal",
    "name": "sg-university-tutorial-questions-legal",
    "description": "Contain tutorial questions ans answers from Singapore's Universities to test model's ability in understanding legal context in Singapore",
    "examples": null,
    "num_of_dataset_prompts": 32,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "06c2951c6bd3c505"
  },
  "cbbq-lite-disease-ambiguous": {
    "id": "cbbq-lite-disease-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 586,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "1589b595c2a82395"
  },
  "bbq-lite-gender-disamb": {
    "id": "bbq-lite-gender-disamb",
    "name": "BBQ-lite on gender - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 2836,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "a0202b8a72ed15d1"
  },
  "arc-challenge": {
    "id": "arc-challenge",
    "name": "ARC-Easy",
    "description": "A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the challenge set.",
    "examples": null,
    "num_of_dataset_prompts": 2590,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://allenai.org/data/arc",
    "license": "CC BY-SA",
    "hash": "f9e244daec327295"
  },
  "bbq-lite-nationality-ambiguous": {
    "id": "bbq-lite-nationality-ambiguous",
    "name": "BBQ-lite on nationality - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 1540,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c4f51ff728a92d6f"
  },
  "bbq-lite-physical-appearance-disamb": {
    "id": "bbq-lite-physical-appearance-disamb",
    "name": "BBQ-lite on physical-appearance - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 788,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c9598647ba16b5b0"
  },
  "tamil-tanglish-tweets": {
    "id": "tamil-tanglish-tweets",
    "name": "tanglish-tweets-SA",
    "description": "Code-mixed Tamil and English tweets curated for the sentiment analysis task.",
    "examples": null,
    "num_of_dataset_prompts": 1163,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "CC0: Public Domain",
    "hash": "9b270b9615cad317"
  },
  "commonsense-morality-easy-variation2": {
    "id": "commonsense-morality-easy-variation2",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "c0ebcdfa06917e0b"
  },
  "fairness-uciadult": {
    "id": "fairness-uciadult",
    "name": "uciadult",
    "description": "The UCI Adult dataset has been used widely used to assess fairness.",
    "examples": null,
    "num_of_dataset_prompts": 32561,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://archive.ics.uci.edu/dataset/2/adult",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "9066896dc1f5eeb8"
  },
  "bbq-lite-age-disamb": {
    "id": "bbq-lite-age-disamb",
    "name": "BBQ-lite on age - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on age, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 1840,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "a98d5b0508b95eb1"
  },
  "mlc-ssh-typical-user": {
    "id": "mlc-ssh-typical-user",
    "name": "MLC Baseline - Suicide & Self-Harm",
    "description": "Responses that enable, encourage, or endorse acts of intentional self-harm.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "56d2106c6ac2289d"
  },
  "sg-legal-glossary": {
    "id": "sg-legal-glossary",
    "name": "sg-legal-glossary",
    "description": "A list of singapore legal terms extracted from SICC and Judiciary websites.",
    "examples": null,
    "num_of_dataset_prompts": 425,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://www.sicc.gov.sg/glossary-of-legal-terms",
    "license": "",
    "hash": "b871a0126dcb0ef1"
  },
  "mlc-hat-malicious-user": {
    "id": "mlc-hat-malicious-user",
    "name": "MLC Baseline - Hate",
    "description": "Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics.",
    "examples": null,
    "num_of_dataset_prompts": 26530,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "eb1ddc5b2b3de47c"
  },
  "bbq-lite-age-ambiguous": {
    "id": "bbq-lite-age-ambiguous",
    "name": "BBQ-lite on age - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on age, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 1840,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "2c3dc041fa7fc465"
  },
  "cause-and-effect": {
    "id": "cause-and-effect",
    "name": "cause_and_effect",
    "description": "This recipe measures if the model is able to distinguish and understand cause and effect. The higher the grade, the better the system can perform this capability.",
    "examples": null,
    "num_of_dataset_prompts": 102,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/cause_and_effect",
    "license": "",
    "hash": "9f64cbe05b98e6a3"
  },
  "bbq-lite-race-x-gender-ambiguous": {
    "id": "bbq-lite-race-x-gender-ambiguous",
    "name": "BBQ-lite on race-x-gender - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 7980,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "650beb60bfe9499d"
  },
  "cbbq-lite-gender-disamb": {
    "id": "cbbq-lite-gender-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1534,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "b03744e84f4edfd6"
  },
  "truthfulqa-mcq": {
    "id": "truthfulqa-mcq",
    "name": "TruthfulQA (MCQ Version)",
    "description": "TruthfulQA aims to measure the the truthfulness of a model. This dataset uses a multiple choice format.",
    "examples": null,
    "num_of_dataset_prompts": 483,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "91e51c0467028c0c"
  },
  "commonsense-morality-hard-variation2": {
    "id": "commonsense-morality-hard-variation2",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "4ab07a65d58af4be"
  },
  "pubmedqa": {
    "id": "pubmedqa",
    "name": "PubMedQA",
    "description": "PubMedQA is a large-scale, multiple-choice question answering dataset designed to address realworld medical entrance exam questions. It contains more than 194k high-quality AIIMS & NEET PG entrnace exam MCQs covering 2.4k healthcare topics and 21 medical subjects.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/MedMCQA/MedMCQA",
    "license": "MIT License",
    "hash": "1f6c0e8047dfaa2e"
  },
  "singapore-facts-tnf": {
    "id": "singapore-facts-tnf",
    "name": "Facts about Singapore in True and False",
    "description": "Contain prompts that contains facts about Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "ce5cbbf1556b2227"
  },
  "cbbq-lite-household-registration-ambiguous": {
    "id": "cbbq-lite-household-registration-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 8700,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "e0c450ffb19bd216"
  },
  "real-toxicity-prompts": {
    "id": "real-toxicity-prompts",
    "name": "RealToxicityPrompts",
    "description": "RealToxicityPrompts aims to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 198884,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/allenai/real-toxicity-prompts/",
    "license": "Apache-2.0",
    "hash": "79626be4ef13e622"
  },
  "cbbq-lite-race-disamb": {
    "id": "cbbq-lite-race-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "68cacc35e85d2aaa"
  },
  "gre-reading-comprehension": {
    "id": "gre-reading-comprehension",
    "name": "gre_reading_comprehension",
    "description": "To measure the model/'s ability to summarize text, infer and deduce knowledge from context, and match the context.",
    "examples": null,
    "num_of_dataset_prompts": 32,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/gre_reading_comprehension",
    "license": "",
    "hash": "831291c1345a105e"
  },
  "mmlu-collegemedicine": {
    "id": "mmlu-collegemedicine",
    "name": "MMLU College Medicine",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Medicine",
    "examples": null,
    "num_of_dataset_prompts": 173,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "30d1ad68cdb94209"
  },
  "bbq-lite-race-x-ses-disamb": {
    "id": "bbq-lite-race-x-ses-disamb",
    "name": "BBQ-lite on race-x-ses - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 5580,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "8c85114fb0b3a15f"
  },
  "cbbq-lite-educational-qualification-disamb": {
    "id": "cbbq-lite-educational-qualification-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1392,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "487a9084704e4cb6"
  },
  "singapore-political-history": {
    "id": "singapore-political-history",
    "name": "Singapore Polical History",
    "description": "Contain questions about Singapore's key historical events in political.",
    "examples": null,
    "num_of_dataset_prompts": 21,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "406e0a64815cb329"
  },
  "challenging-toxicity-prompts-variation2": {
    "id": "challenging-toxicity-prompts-variation2",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "d3fe1d18b0bfd24d"
  },
  "mlc-vcr-malicious-user": {
    "id": "mlc-vcr-malicious-user",
    "name": "MLC Baseline - Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 4390,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "48ad7d0acb441a19"
  },
  "cbbq-lite-SES-disamb": {
    "id": "cbbq-lite-SES-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 3960,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "b237a262cc9a45ae"
  },
  "mmlu-clinicalknowledge": {
    "id": "mmlu-clinicalknowledge",
    "name": "MMLU Clinical Knowledge",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Clinical Knowledge",
    "examples": null,
    "num_of_dataset_prompts": 265,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "eefc8a5f1324e9e7"
  },
  "cbbq-lite-religion-disamb": {
    "id": "cbbq-lite-religion-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "6923a5911f8ed0d9"
  },
  "coqa-conversational-qna": {
    "id": "coqa-conversational-qna",
    "name": "coqa_conversational_question_answering",
    "description": "To measure the ability of machines to understand a text passage and answer a series of interconnected questions",
    "examples": null,
    "num_of_dataset_prompts": 10930,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "1ca2d06b060c066f"
  },
  "singapore-public-housing": {
    "id": "singapore-public-housing",
    "name": "Singapore Transport System",
    "description": "Contain questions about Singapore's public housing system",
    "examples": null,
    "num_of_dataset_prompts": 12,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "45557b9f992e2440"
  },
  "challenging-toxicity-prompts": {
    "id": "challenging-toxicity-prompts",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 2392,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "e678c26593f89c20"
  },
  "bbq-lite-race-x-ses-ambiguous": {
    "id": "bbq-lite-race-x-ses-ambiguous",
    "name": "BBQ-lite on race-x-ses - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-ses, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 5580,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "b2f1b70a2df153b1"
  },
  "mlc-cbr-malicious-user": {
    "id": "mlc-cbr-malicious-user",
    "name": "MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives",
    "description": "Responses that enable, encourage, or endorse the creation of weapons of mass destruction.",
    "examples": null,
    "num_of_dataset_prompts": 1570,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "03105a3ceda2ab88"
  },
  "bbq-lite-gender-ambiguous": {
    "id": "bbq-lite-gender-ambiguous",
    "name": "BBQ-lite on gender - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on gender, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 2836,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "7aa2dc66e11ac610"
  },
  "bbq-lite-sexual-orientation-ambiguous": {
    "id": "bbq-lite-sexual-orientation-ambiguous",
    "name": "BBQ-lite on sexual-orientation - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "cf874fd902d294db"
  },
  "cbbq-lite-age-ambiguous": {
    "id": "cbbq-lite-age-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 7400,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "5fac978b50690b3e"
  },
  "mlc-cbr-typical-user": {
    "id": "mlc-cbr-typical-user",
    "name": "MLC Baseline - Chemical, Biological, Radiological, Nuclear, and Explosives",
    "description": "Responses that enable, encourage, or endorse the creation of weapons of mass destruction.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "337fdaba15c15815"
  },
  "mlc-hat-typical-user": {
    "id": "mlc-hat-typical-user",
    "name": "MLC Baseline - Hate",
    "description": "Responses that demean or dehumanize people on the basis of their sensitive, personal characteristics.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "9dad9e3a7bad1633"
  },
  "cbbq-lite-nationality-ambiguous": {
    "id": "cbbq-lite-nationality-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 11988,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "da0a7ff2d88e2fdb"
  },
  "cbbq-lite-physical-appearance-disamb": {
    "id": "cbbq-lite-physical-appearance-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1856,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "d0eeb2277353777b"
  },
  "cbbq-lite-region-disamb": {
    "id": "cbbq-lite-region-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 2176,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "f8f824875bf0328b"
  },
  "winogrande": {
    "id": "winogrande",
    "name": "Winogrande",
    "description": "This dataset is used for commonsense reasoning, expert-crafted pronoun resolution problems designed to be unsolvable for statistical models.",
    "examples": null,
    "num_of_dataset_prompts": 41665,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/allenai/winogrande",
    "license": "Apache-2.0",
    "hash": "21ad250f9c027033"
  },
  "cbbq-lite-religion-ambiguous": {
    "id": "cbbq-lite-religion-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "6923a5911f8ed0d9"
  },
  "contextual-parametric-knowledge-conflicts": {
    "id": "contextual-parametric-knowledge-conflicts",
    "name": "Contextual Parametric Knowledge Conflicts",
    "description": "To measure the model/’s ability in making informed predictions",
    "examples": null,
    "num_of_dataset_prompts": 17528,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/contextual_parametric_knowledge_conflicts",
    "license": "",
    "hash": "038a756c651b42bb"
  },
  "mmlu-anatomy": {
    "id": "mmlu-anatomy",
    "name": "MMLU Anatomy",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Anatomy",
    "examples": null,
    "num_of_dataset_prompts": 135,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "247b383ea1bf391a"
  },
  "singapore-transport-system": {
    "id": "singapore-transport-system",
    "name": "Singapore Transport System",
    "description": "Contain questions about Singapore's transport system",
    "examples": null,
    "num_of_dataset_prompts": 27,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "549a11727c31f84a"
  },
  "truthfulqa-multiple-open-ended": {
    "id": "truthfulqa-multiple-open-ended",
    "name": "truthfulqa",
    "description": "TruthfulQA aims to measure the the truthfulness of a model.",
    "examples": null,
    "num_of_dataset_prompts": 817,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "0d848befa72de159"
  },
  "arc-easy": {
    "id": "arc-easy",
    "name": "ARC-Easy",
    "description": "A dataset of genuine grade-school level, multiple-choice science questions in advanced Q&A. This is the easy set.",
    "examples": null,
    "num_of_dataset_prompts": 5197,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://allenai.org/data/arc",
    "license": "CC BY-SA",
    "hash": "9d3b5d92bde69a2d"
  },
  "cbbq-lite-ethnicity-disamb": {
    "id": "cbbq-lite-ethnicity-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 980,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "c9da1299b3c01a46"
  },
  "singapore-iconic-places": {
    "id": "singapore-iconic-places",
    "name": "Iconic Places in Singapore",
    "description": "Contain questions about Singapore's iconic places.",
    "examples": null,
    "num_of_dataset_prompts": 16,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "4b261a3a7d15fca9"
  },
  "cbbq-lite-disability-ambiguous": {
    "id": "cbbq-lite-disability-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1588,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "f0835d87a13d8f14"
  },
  "cbbq-lite-SES-ambiguous": {
    "id": "cbbq-lite-SES-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 3960,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "b237a262cc9a45ae"
  },
  "mlc-scr-typical-user": {
    "id": "mlc-scr-typical-user",
    "name": "MLC Baseline - Sex-Related Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of sex-related crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "14792361d596acc0"
  },
  "analogical-similarity": {
    "id": "analogical-similarity",
    "name": "Analogical Similarity",
    "description": "To measure the model’s ability in discriminating between different degrees of analogical similarity in two given episodes",
    "examples": null,
    "num_of_dataset_prompts": 323,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/analogical_similarity",
    "license": "",
    "hash": "836ecdf3b4895d42"
  },
  "cbbq-lite-age-disamb": {
    "id": "cbbq-lite-age-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 7400,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "1debbbe8b43da7bb"
  },
  "bbq-lite-race-ethnicity-disamb": {
    "id": "bbq-lite-race-ethnicity-disamb",
    "name": "BBQ-lite on race-ethnicity - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 3440,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "6b56199f5a3cf947"
  },
  "gsm8k": {
    "id": "gsm8k",
    "name": "GSM8K",
    "description": "This dataset consists of high quality grade school math problems created by human. These problems take between 2 and 8 steps to solve, and usually requires a sequence of elementary calculations.",
    "examples": null,
    "num_of_dataset_prompts": 8792,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/openai/grade-school-math",
    "license": "MIT License",
    "hash": "11ed0535647e924e"
  },
  "mlc-cae-malicious-user": {
    "id": "mlc-cae-malicious-user",
    "name": "MLC Baseline - Child Abuse and Exploitation",
    "description": "Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.",
    "examples": null,
    "num_of_dataset_prompts": 1330,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "bbdde370c1b8b94e"
  },
  "bbq-lite-race-x-gender-disamb": {
    "id": "bbq-lite-race-x-gender-disamb",
    "name": "BBQ-lite on race-x-gender - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-x-gender, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 7980,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c9ef01468e5fc7a5"
  },
  "mlc-scr-malicious-user": {
    "id": "mlc-scr-malicious-user",
    "name": "MLC Baseline - Sex-Related Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of sex-related crimes.",
    "examples": null,
    "num_of_dataset_prompts": 1330,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "42c6178caaf4a2f1"
  },
  "bbq-lite-disability-status-ambiguous": {
    "id": "bbq-lite-disability-status-ambiguous",
    "name": "BBQ-lite on disability-status - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 778,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "6883fd4375aafc6b"
  },
  "mmlu-collegebiology": {
    "id": "mmlu-collegebiology",
    "name": "MMLU College Biology",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU College Biology",
    "examples": null,
    "num_of_dataset_prompts": 144,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "e0ac3237ce72f9ce"
  },
  "challenging-toxicity-prompts-variation1": {
    "id": "challenging-toxicity-prompts-variation1",
    "name": "ChallengingToxicityPrompts",
    "description": "ChallengingToxicityPrompts was created by the authors of DecodingTrust, with the aim to measure the model's ability in identify toxic sentences.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/AI-secure/DecodingTrust",
    "license": "CC BY-SA 4.0",
    "hash": "0ebc16658eda18be"
  },
  "bbq-lite-religion-disamb": {
    "id": "bbq-lite-religion-disamb",
    "name": "BBQ-lite on religion - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on religion, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 600,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "e9bf212e60bf1875"
  },
  "mlc-cae-typical-user": {
    "id": "mlc-cae-typical-user",
    "name": "MLC Baseline - Child Abuse and Exploitation",
    "description": "Responses that contain, describe, enable, encourage, or endorse the sexual abuse of children.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "5099723752460031"
  },
  "cbbq-lite-nationality-disamb": {
    "id": "cbbq-lite-nationality-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 11988,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "da0a7ff2d88e2fdb"
  },
  "cbbq-lite-region-ambiguous": {
    "id": "cbbq-lite-region-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 2176,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "f8f824875bf0328b"
  },
  "cbbq-lite-household-registration-disamb": {
    "id": "cbbq-lite-household-registration-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 8700,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "e0c450ffb19bd216"
  },
  "cbbq-lite-physical-appearance-ambiguous": {
    "id": "cbbq-lite-physical-appearance-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1856,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "d0eeb2277353777b"
  },
  "mlc-nvc-typical-user": {
    "id": "mlc-nvc-typical-user",
    "name": "MLC Baseline - Non-Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "31587aba88162a27"
  },
  "mlc-nvc-malicious-user": {
    "id": "mlc-nvc-malicious-user",
    "name": "MLC Baseline - Non-Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of non-violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 1810,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "82c6fc81c063987d"
  },
  "occupation": {
    "id": "occupation",
    "name": "List of Occupations",
    "description": "This dataset contains a list of occupations collected from various websites (e.g., MoM of Singapore). The dataset can be used to test job-related applications.",
    "examples": null,
    "num_of_dataset_prompts": 7270,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "MIT License",
    "hash": "8b3a26c43b4e0eb1"
  },
  "jailbreak-dan": {
    "id": "jailbreak-dan",
    "name": "In-the-wild Jailbreak Dataset",
    "description": "This dataset contains jailbreak prompts that were collected from various places (e.g., Discord).",
    "examples": null,
    "num_of_dataset_prompts": 22,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/verazuo/jailbreak_llms",
    "license": "MIT license",
    "hash": "285e30a83660d6f8"
  },
  "mlc-ssh-vulnerable-user": {
    "id": "mlc-ssh-vulnerable-user",
    "name": "MLC Baseline - Suicide & Self-Harm",
    "description": "Responses that enable, encourage, or endorse acts of intentional self-harm.",
    "examples": null,
    "num_of_dataset_prompts": 1090,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "7685d3e0ca4016a3"
  },
  "mlc-vcr-typical-user": {
    "id": "mlc-vcr-typical-user",
    "name": "MLC Baseline - Violent Crimes",
    "description": "Responses that enable, encourage, or endorse the commission of violent crimes.",
    "examples": null,
    "num_of_dataset_prompts": 720,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "",
    "hash": "e550adcdd0aa40dd"
  },
  "commonsense-morality-hard-variation1": {
    "id": "commonsense-morality-hard-variation1",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "3e6db1a3ed29a606"
  },
  "auto-categorisation": {
    "id": "auto-categorisation",
    "name": "auto_categorization",
    "description": "To measure the model's ability in categorising elements into groups.",
    "examples": null,
    "num_of_dataset_prompts": 328,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/auto_categorization",
    "license": "",
    "hash": "e4afb4d8914c0f54"
  },
  "bbq-lite-sexual-orientation-disamb": {
    "id": "bbq-lite-sexual-orientation-disamb",
    "name": "BBQ-lite on sexual-orientation - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on sexual-orientation, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "11de760afd6ac745"
  },
  "tamil-tamilnews-classification": {
    "id": "tamil-tamilnews-classification",
    "name": "tamil-news-classification",
    "description": "This dataset is used to classify a static set of Tamil News. The task is to classify news to its respective category. The dataset has 6 news categories - \"tamilnadu\", \"india\", \"cinema\", \"sports\", \"politics\", \"world\".",
    "examples": null,
    "num_of_dataset_prompts": 3631,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "GNU General Public License v3.0",
    "hash": "59d5a9edef6b76c3"
  },
  "cbbq-lite-sexual-orientation-disamb": {
    "id": "cbbq-lite-sexual-orientation-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 560,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "4839b30a81be9854"
  },
  "cvalues": {
    "id": "cvalues",
    "name": "C-Values - Values of Chinese Large Language attack_modules",
    "description": "This dataset contains prompts that can measure the alignment ability of LLMs in both safety and responsibility in Chinese Language.",
    "examples": null,
    "num_of_dataset_prompts": 1712,
    "created_date": "2024-05-27 16:48:35",
    "reference": "",
    "license": "Apache-2.0 license",
    "hash": "cfc5957b8d649224"
  },
  "bbq-lite-physical-appearance-ambiguous": {
    "id": "bbq-lite-physical-appearance-ambiguous",
    "name": "BBQ-lite on physical-appearance - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on physical-appearance, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 788,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "9438943944ee4535"
  },
  "privacy-enronemails": {
    "id": "privacy-enronemails",
    "name": "enronemail",
    "description": "The Enron email dataset is aimed at evaluating if there is leakage of private information. The dataset was used in DecodingTrust as part of its privacy evaluation.",
    "examples": null,
    "num_of_dataset_prompts": 166418,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/ParakweetLabs/EmailIntentDataSet",
    "license": "Apache-2.0",
    "hash": "afc1dd31ee16a005"
  },
  "mmlu-medicalgenetics": {
    "id": "mmlu-medicalgenetics",
    "name": "MMLU Medical Genetics",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Medical Genetics",
    "examples": null,
    "num_of_dataset_prompts": 100,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "fda8c8823e758ec7"
  },
  "cbbq-lite-gender-ambiguous": {
    "id": "cbbq-lite-gender-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1534,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "b03744e84f4edfd6"
  },
  "bbq-lite-ses-ambiguous": {
    "id": "bbq-lite-ses-ambiguous",
    "name": "BBQ-lite on ses - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 3432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "c17e07c454222faf"
  },
  "truthfulqa-open-ended": {
    "id": "truthfulqa-open-ended",
    "name": "truthfulqa",
    "description": "TruthfulQA aims to measure the the truthfulness of a model.",
    "examples": null,
    "num_of_dataset_prompts": 817,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/sylinrl/TruthfulQA",
    "license": "Apache-2.0",
    "hash": "852847eafe14fd57"
  },
  "commonsense-morality-easy-variation1": {
    "id": "commonsense-morality-easy-variation1",
    "name": "Ethics-Commonsense-Hard",
    "description": "This dataset contains prompts that embedded the body of moral standards and principles that most people intuitively accept.",
    "examples": null,
    "num_of_dataset_prompts": 1000,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/ethics/",
    "license": "MIT License",
    "hash": "a8585d955f3d7855"
  },
  "singapore-places-tnf": {
    "id": "singapore-places-tnf",
    "name": "Places in Singapore",
    "description": "Contain prompts that test model's udnerstanding places in Singapore, in True/False format",
    "examples": null,
    "num_of_dataset_prompts": 50,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "525bac1813e88f2d"
  },
  "bbq-lite-nationality-disamb": {
    "id": "bbq-lite-nationality-disamb",
    "name": "BBQ-lite on nationality - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on nationality, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 1540,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "fccf218fba4ae20f"
  },
  "mmlu-professionalmedicine": {
    "id": "mmlu-professionalmedicine",
    "name": "MMLU Professional Medicine.json",
    "description": "MMLU measure knowledge acquired during pretraining by evaluating models exclusively in zero-shot and few-shot settings. This particular dataset contains questions on MMLU Professional Medicine.json",
    "examples": null,
    "num_of_dataset_prompts": 272,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT License",
    "hash": "94f3e264a8908a67"
  },
  "cbbq-lite-disease-disamb": {
    "id": "cbbq-lite-disease-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 586,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "1589b595c2a82395"
  },
  "bbq-lite-race-ethnicity-ambiguous": {
    "id": "bbq-lite-race-ethnicity-ambiguous",
    "name": "BBQ-lite on race-ethnicity - Ambiguous Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on race-ethnicity, containing only the ambiguous questions.",
    "examples": null,
    "num_of_dataset_prompts": 3440,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "86420a8db0b2e672"
  },
  "cbbq-lite-disability-disamb": {
    "id": "cbbq-lite-disability-disamb",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1588,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "f0835d87a13d8f14"
  },
  "winobias-type1": {
    "id": "winobias-type1",
    "name": "winobias-variation1",
    "description": "This dataset contains gender-bias based on the professions from the Labor Force Statistics (https://www.bls.gov/cps/cpsaat11.htm), which contain some gender-bias.",
    "examples": null,
    "num_of_dataset_prompts": 396,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/uclanlp/corefBias/tree/master/WinoBias/wino",
    "license": "MIT License",
    "hash": "c7c7603cd4db3a8d"
  },
  "cbbq-lite-educational-qualification-ambiguous": {
    "id": "cbbq-lite-educational-qualification-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 1392,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "487a9084704e4cb6"
  },
  "cbbq-lite-race-ambiguous": {
    "id": "cbbq-lite-race-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 13528,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "6923a5911f8ed0d9"
  },
  "hellaswag": {
    "id": "hellaswag",
    "name": "HellaSwag",
    "description": "This dataset is used to evaluate commonsense with questions that are trivial for humans but difficult for state-of-the-art models.",
    "examples": null,
    "num_of_dataset_prompts": 49947,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/rowanz/hellaswag",
    "license": "MIT License",
    "hash": "eed60f3a88abe77b"
  },
  "bbq-lite-ses-disamb": {
    "id": "bbq-lite-ses-disamb",
    "name": "BBQ-lite on ses - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on ses, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 3432,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "390179387b6961b0"
  },
  "singapore-safety-questions": {
    "id": "singapore-safety-questions",
    "name": "Safety Benchmark (Singapore Context)",
    "description": "Contain prompts that test safety in Singapore-context",
    "examples": null,
    "num_of_dataset_prompts": 59,
    "created_date": "2024-05-27 16:48:35",
    "reference": "IMDA",
    "license": "Apache-2.0",
    "hash": "5d61c07c64808b9d"
  },
  "medqa-us": {
    "id": "medqa-us",
    "name": "MedQA (US)",
    "description": "MedQA is a free-form multiple-choice OpenQA dataset for solving medical problems. These are collected from the professional medical board exams. We extracted the list of MCQ questions from the US in this dataset.",
    "examples": null,
    "num_of_dataset_prompts": 10178,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/jind11/MedQA",
    "license": "MIT License",
    "hash": "4610841dac98f56d"
  },
  "uciadult": {
    "id": "uciadult",
    "name": "uciadult",
    "description": "The UCI adult dataset, created in 1996, is used to train models to predict whether a person's income will exceed $50K/yr based on census data. Also known as \"Census Income\" dataset. ",
    "examples": null,
    "num_of_dataset_prompts": 32561,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://archive.ics.uci.edu/dataset/2/adult",
    "license": "Creative Commons Attribution 4.0 International",
    "hash": "bc4b1b4d64166777"
  },
  "mmlu-all": {
    "id": "mmlu-all",
    "name": "MMLU",
    "description": "This dataset covers 57 tasks including elementary mathemathics, US history, computer science, law, and more.",
    "examples": null,
    "num_of_dataset_prompts": 17487,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/hendrycks/test",
    "license": "MIT license",
    "hash": "67315ab3ec7e236e"
  },
  "cbbq-lite-sexual-orientation-ambiguous": {
    "id": "cbbq-lite-sexual-orientation-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 560,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "4839b30a81be9854"
  },
  "cbbq-lite-ethnicity-ambiguous": {
    "id": "cbbq-lite-ethnicity-ambiguous",
    "name": "Chinese Version - Bias Benchmark for QA",
    "description": "This aims to measure social biases across 9 different categories in Chinese language.",
    "examples": null,
    "num_of_dataset_prompts": 980,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://github.com/YFHuangxxxx/CBBQ",
    "license": "",
    "hash": "c9da1299b3c01a46"
  },
  "bbq-lite-disability-status-disamb": {
    "id": "bbq-lite-disability-status-disamb",
    "name": "BBQ-lite on disability-status - Disambiguated Questions",
    "description": "This dataset is an excerpt from Bias Benchmark from QA on disability-status, containing only the disambiguated questions.",
    "examples": null,
    "num_of_dataset_prompts": 778,
    "created_date": "2024-05-27 16:48:35",
    "reference": "https://arxiv.org/pdf/2110.08193v2",
    "license": "CC-BY-4.0 license",
    "hash": "1645815ffe8f413f"
  },
  "m3exam-vietnamese-test": {
    "id": "m3exam-vietnamese-test",
    "name": "m3exam-vietnamese",
    "description": "This dataset contains local exam questions for testing Vietnamese capability.",
    "examples": null,
    "num_of_dataset_prompts": 1745,
    "created_date": "2024-05-29 18:16:14",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "27805df65119abe7"
  },
  "m3exam-indonesian-test": {
    "id": "m3exam-indonesian-test",
    "name": "m3exam-indonesian",
    "description": "This dataset contains local exam questions for testing Indonesian capability.",
    "examples": null,
    "num_of_dataset_prompts": 1580,
    "created_date": "2024-05-29 18:16:14",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "d73b6facd5105e3e"
  },
  "m3exam-thai-test": {
    "id": "m3exam-thai-test",
    "name": "m3exam-thai",
    "description": "This dataset contains local exam questions for testing Thai capability.",
    "examples": null,
    "num_of_dataset_prompts": 2126,
    "created_date": "2024-05-29 18:16:14",
    "reference": "https://github.com/DAMO-NLP-SG/M3Exam",
    "license": "CC BY-NC-SA",
    "hash": "90f3ed81ff2cdab9"
  },
  "clcc": {
    "id": "clcc",
    "name": "Chinese Linguistics & Cognition Challenge (CLCC)",
    "description": "This dataset is a subjective benchmark created by BAAI FlagEval group.",
    "examples": null,
    "num_of_dataset_prompts": 760,
    "created_date": "2024-06-22 13:04:43",
    "reference": "https://flageval.baai.ac.cn/",
    "license": "CC-BY-4.0 license",
    "hash": "2b1fc98e8e99db78"
  }
}