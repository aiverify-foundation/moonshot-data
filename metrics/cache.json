{
  "bertscore": {
    "id": "bertscore",
    "name": "BertScore",
    "description": "BertScore uses Bert to check for the similarity in embedding between two sentences.",
    "endpoints": [],
    "configurations": {},
    "hash": "eb5070d4539d4fc1"
  },
  "spelling": {
    "id": "spelling",
    "name": "SpellingScore",
    "description": "SpellingScore uses Levenshetein Distance to find permutations within an edit distance of 2 form the original word before comparing to known words in a word frequency list.",
    "endpoints": [],
    "configurations": {},
    "hash": "950f481fd184bd52"
  },
  "gpt4annotator": {
    "id": "gpt4annotator",
    "name": "GPT4 Annotator",
    "description": "Calculates the number of refused response using GPT4.",
    "endpoints": [
      "llm-judge-openai-gpt4-annotator"
    ],
    "configurations": {},
    "hash": "eeff9896775ffc05"
  },
  "bleuscore": {
    "id": "bleuscore",
    "name": "BleuScore",
    "description": "Bleuscore uses Bleu to return the various rouge scores.",
    "endpoints": [],
    "configurations": {},
    "hash": "ed9b936ba75a62e3"
  },
  "readabilityscore": {
    "id": "readabilityscore",
    "name": "ReadabilityScore",
    "description": "ReadabilityScore uses Flesch Reading Ease to compute the complexity of the output",
    "endpoints": [],
    "configurations": {},
    "hash": "ae967a8551e1fb71"
  },
  "toxicity-classifier": {
    "id": "toxicity-classifier",
    "name": "Toxicity Classifier",
    "description": "This classifier measures how toxic a given input isand calculate the number of toxic sentence detected.",
    "endpoints": [],
    "configurations": {},
    "hash": "2eca0a01669516f8"
  },
  "rougescorer": {
    "id": "rougescorer",
    "name": "RougeScorer",
    "description": "RougeScorer returns the various rouge scores.",
    "endpoints": [],
    "configurations": {},
    "hash": "0c445851def19621"
  },
  "advglue": {
    "id": "advglue",
    "name": "Attack Success Rate",
    "description": "Attack success rate measures how successful a changed prompt performs. A high score shows that the system under test is highly sensitive towards a prompt with minimal changes.",
    "endpoints": [],
    "configurations": {},
    "hash": "25b968693a68bdac"
  },
  "leakagerate": {
    "id": "leakagerate",
    "name": "LeakageRate",
    "description": "Leakage Rate will compare the LCS between two string - Output and Target.",
    "endpoints": [],
    "configurations": {},
    "hash": "54cf0c39629b60c3"
  },
  "reverseexactstrmatch": {
    "id": "reverseexactstrmatch",
    "name": "ReverseExactStrMatch",
    "description": "ReverseExactStrMatch will compare the output from language model with the expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "ad74fe06508040af"
  },
  "exactstrmatch": {
    "id": "exactstrmatch",
    "name": "ExactStrMatch",
    "description": "ExactStrMatch will compare the output from language model with a single target or multiple expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "6bc2f6c84b3304b9"
  },
  "relaxstrmatch": {
    "id": "relaxstrmatch",
    "name": "RelaxStrMatch",
    "description": "RelaxStrMatch will remove symbols and spaces before comparing the output from language model with the expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "d236bd2e45701bde"
  },
  "llamaguardannotator": {
    "id": "llamaguardannotator",
    "name": "LlamaGuard Annotator",
    "description": "Calculates the number of safe, unsafe and unknown response by using LlamaGuard.",
    "endpoints": [
      "together-llama-guard-7b-assistant"
    ],
    "configurations": {},
    "hash": "017523d1c1b18390"
  },
  "samplemetric": {
    "id": "samplemetric",
    "name": "SampleMetric",
    "description": "Sample Metric will provide examples on connecting to LLMs.",
    "endpoints": [
      "openai-gpt35-turbo-16k",
      "openai-gpt35-turbo"
    ],
    "configurations": {
      "threshold_value": "0.35",
      "num_of_prompts_to_calculate": 1
    },
    "hash": "03ba1f69faf76a1c"
  },
  "lionguardclassifier": {
    "id": "lionguardclassifier",
    "name": "LionGuard Classifier",
    "description": "This classifier measures how unsafe a given input is in the Singapore context.\nIt can calculate the number of toxic sentences detected. ",
    "endpoints": [],
    "configurations": {
      "grading_classifier": "toxic"
    },
    "hash": "efa4a2b0497f79d2"
  },
  "flagjudgeannotator": {
    "id": "flagjudgeannotator",
    "name": "FlagJudge Annotator",
    "description": "Calculates the number of correct response using FlagJudge (A judge model from FlagEval Group).",
    "endpoints": [
      "flageval-flagjudge"
    ],
    "configurations": {},
    "hash": "83d7aedea043c632"
  },
  "q16_metric": {
    "id": "q16_metric",
    "name": "Q16Metric",
    "description": "Q16 is a binary classifier used to determine whether an image is inappropriate (https://github.com/ml-research/Q16)",
    "endpoints": [],
    "configurations": {},
    "hash": "5c11f26e9288b4a4"
  },
  "nudenet_metric": {
    "id": "nudenet_metric",
    "name": "NudeNetMetric",
    "description": "NudeNet is a classifier used to deduce probabilities of exposed body parts in images.",
    "endpoints": [],
    "configurations": {},
    "hash": "34e31007f555626c"
  },
  "genderbias_metric": {
    "id": "genderbias_metric",
    "name": "GenderBiasMetric",
    "description": "This metric uses a CLIP based gender classifier to determine the proportion of females returning a score between 0 and 0.5. score = |female_frac - 0.5|",
    "endpoints": [],
    "configurations": {},
    "hash": "279f6ab10076eea5"
  },
  "faithfulness": {
    "id": "faithfulness",
    "name": "Faithfulness",
    "description": "This measures the factual consistency of the generated answer against the given context. It is calculated from answer and retrieved context. The answer is scaled to (0,1) range. Higher the better.",
    "endpoints": [
      "azure-langchain-openai-chatopenai-gpt4o"
    ],
    "configurations": {
      "embeddings": [
        "azure-langchain-openai-embedding-ada-2"
      ]
    },
    "hash": "ad0b43142ff78095"
  },
  "answercorrectness": {
    "id": "answercorrectness",
    "name": "AnswerCorrectness",
    "description": "Answer correctness in the context of Ragas involves evaluating the accuracy of a generated answer compared to the ground truth. This process assesses both the semantic and factual similarities between the answer and the ground truth. Scores range from 0 to 1, where a higher score indicates a closer match, thus higher correctness.",
    "endpoints": [
      "azure-langchain-openai-chatopenai-gpt4o"
    ],
    "configurations": {
      "embeddings": [
        "azure-langchain-openai-embedding-ada-2"
      ]
    },
    "hash": "c6c50e11ad8ebfb9"
  },
  "contextrecall": {
    "id": "contextrecall",
    "name": "ContextRecall",
    "description": "Context recall measures the extent to which the retrieved context aligns with the annotated answer, treated as the ground truth. It is computed using question, ground truth and the retrieved context, and the values range between 0 and 1, with higher values indicating better performance.",
    "endpoints": [
      "azure-langchain-openai-chatopenai-gpt4o"
    ],
    "configurations": {
      "embeddings": [
        "azure-langchain-openai-embedding-ada-2"
      ]
    },
    "hash": "b1249a592f9b75bb"
  },
  "contextprecision": {
    "id": "contextprecision",
    "name": "ContextPrecision",
    "description": "Context Precision is a metric that evaluates whether all of the ground-truth relevant items present in the contexts are ranked higher or not. Ideally, all the relevant chunks must appear at the top ranks. This metric is computed using the question, ground_truth, and the contexts, with values ranging between 0 and 1, where higher scores indicate better precision.",
    "endpoints": [
      "azure-langchain-openai-chatopenai-gpt4o"
    ],
    "configurations": {
      "embeddings": [
        "azure-langchain-openai-embedding-ada-2"
      ]
    },
    "hash": "8964bf260c3fd34e"
  },
  "answerrelevance": {
    "id": "answerrelevance",
    "name": "AnswerRelevance",
    "description": "The evaluation metric, Answer Relevancy, focuses on assessing how pertinent the generated answer is to the given prompt. A lower score is assigned to answers that are incomplete or contain redundant information and higher scores indicate better relevancy. This metric is computed using the question, the context and the answer.",
    "endpoints": [
      "azure-langchain-openai-chatopenai-gpt4o"
    ],
    "configurations": {
      "embeddings": [
        "azure-langchain-openai-embedding-ada-2"
      ]
    },
    "hash": "3a16362073d037e6"
  },
  "cybersecevalannotator": {
    "id": "cybersecevalannotator",
    "name": "General Judge LLM",
    "description": "Calculates number of yes and no replied by judge llm  (yes means prompt injection succeed)",
    "endpoints": [
      "llm-judge-azure-gpt4-annotator"
    ],
    "configurations": {},
    "hash": "80729a1936c4624c"
  },
  "exactstrmatch-f1-squad-v2": {
    "id": "exactstrmatch-f1-squad-v2",
    "name": "MyF1Score",
    "description": "ExactStrMatch will compare the output from language model with a single target or multiple expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "e7278406cdc9883f"
  },
  "exactstrmatch-gsm8k-test": {
    "id": "exactstrmatch-gsm8k-test",
    "name": "MyExactStrMatch",
    "description": "ExactStrMatch will compare the output from language model with a single target or multiple expected target.",
    "endpoints": [],
    "configurations": {},
    "hash": "fd18e592751d265f"
  }
}